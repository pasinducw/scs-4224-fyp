
@misc{14AudioProgrammer,
  title = {(14) {{The Audio Programmer}} - {{YouTube}}},
  howpublished = {https://www.youtube.com/c/TheAudioProgrammer/videos},
  file = {/Users/pasinduwijesena/Zotero/storage/AU36XW9X/videos.html}
}

@misc{14MachineLearning,
  title = {(14) {{Machine Learning}} with {{Phil}} - {{YouTube}}},
  howpublished = {https://www.youtube.com/c/MachineLearningwithPhil/videos},
  file = {/Users/pasinduwijesena/Zotero/storage/CPCSY2AT/videos.html}
}

@misc{14TwoMinute,
  title = {(14) {{Two Minute Music Theory}} - {{YouTube}}},
  howpublished = {https://www.youtube.com/c/TwoMinuteMusicTheory/videos},
  file = {/Users/pasinduwijesena/Zotero/storage/ZWZFZU66/videos.html}
}

@misc{2020MIREX2020Results,
  title = {2020:{{MIREX2020 Results}} - {{MIREX Wiki}}},
  howpublished = {https://www.music-ir.org/mirex/wiki/2020:MIREX2020\_Results}
}

@misc{577SteveBrunton,
  title = {(577) {{Steve Brunton}} - {{YouTube}}},
  abstract = {Enjoy the videos and music you love, upload original content, and share it all with friends, family, and the world on YouTube.},
  howpublished = {https://www.youtube.com/},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/QVS2GKIZ/UCm5mt-A4w61lknZ9lCsZtBw.html}
}

@misc{701YouTube,
  title = {(701) {{YouTube}}},
  abstract = {Enjoy the videos and music you love, upload original content, and share it all with friends, family, and the world on YouTube.},
  howpublished = {https://www.youtube.com/},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/CI8RH9Q2/www.youtube.com.html}
}

@misc{70MusicTechnologyGroupYouTube,
  title = {(70) {{MusicTechnologyGroup}} - {{YouTube}}},
  howpublished = {https://www.youtube.com/c/MusicTechnologyGroup/videos},
  file = {/Users/pasinduwijesena/Zotero/storage/29MWZJ3M/videos.html}
}

@misc{88ValerioVelardo,
  title = {(88) {{Valerio Velardo}} - {{The Sound}} of {{AI}} - {{YouTube}}},
  abstract = {Enjoy the videos and music you love, upload original content, and share it all with friends, family, and the world on YouTube.},
  howpublished = {https://www.youtube.com/},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/FGP7JQ3K/UCZPFjMe1uRSirmSpznqvJfQ.html}
}

@article{arcas_now_2017,
  title = {Now {{Playing}}: {{Continuous}} Low-Power Music Recognition},
  shorttitle = {Now {{Playing}}},
  author = {y Arcas, Blaise Ag{\"u}era and Gfeller, Beat and Guo, Ruiqi and Kilgour, Kevin and Kumar, Sanjiv and Lyon, James and Odell, Julian and Ritter, Marvin and Roblek, Dominik and Sharifi, Matthew and Velimirovi{\'c}, Mihajlo},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.10958 [cs, eess]},
  eprint = {1711.10958},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Existing music recognition applications require a connection to a server that performs the actual recognition. In this paper we present a low-power music recognizer that runs entirely on a mobile device and automatically recognizes music without user interaction. To reduce battery consumption, a small music detector runs continuously on the mobile device's DSP chip and wakes up the main application processor only when it is confident that music is present. Once woken, the recognizer on the application processor is provided with a few seconds of audio which is fingerprinted and compared to the stored fingerprints in the on-device fingerprint database of tens of thousands of songs. Our presented system, Now Playing, has a daily battery usage of less than 1\% on average, respects user privacy by running entirely on-device and can passively recognize a wide range of music.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/pasinduwijesena/Zotero/storage/7ZZ2NADW/Arcas et al. - 2017 - Now Playing Continuous low-power music recognitio.pdf;/Users/pasinduwijesena/Zotero/storage/3R6IV3GH/1711.html}
}

@article{baez_suarez_unsupervised_2020,
  title = {Unsupervised {{Deep Learning Recurrent Model}} for {{Audio Fingerprinting}}},
  author = {B{\'a}ez Su{\'a}rez, Abraham},
  year = {2020},
  month = apr,
  publisher = {{Instituto Tecnol\'ogico y de Estudios Superiores de Monterrey}},
  doi = {10.1145/3380828},
  abstract = {Audio fingerprinting techniques were developed to index and retrieve audio samples by comparing a content-based compact signature of the audio instead of the entire audio sample, thereby reducing memory and computational expense. Different techniques have been applied to create audio fingerprints, however, with the introduction of deep learning, new data-driven unsupervised approaches are available.    This doctoral dissertation presents a Sequence-to-Sequence Autoencoder Model for Audio Fingerprinting (SAMAF) which improved hash generation through a novel loss function composed of terms: Mean Square Error, minimizing the reconstruction error; Hash Loss, minimizing the distance between similar hashes and encouraging clustering; and Bitwise Entropy Loss, minimizing the variation inside the clusters.    The performance of the model was assessed with a subset of VoxCeleb1 dataset, a "speech in-the-wild" dataset. Furthermore, the model was compared against three baselines: Dejavu, a Shazam-like algorithm; Robust Audio Fingerprinting System  (RAFS), a Bit Error Rate (BER) methodology robust to time-frequency distortions and coding/decoding transformations; and Panako, a constellation algorithm-based adding time-frequency distortion resilience.    Extensive empirical evidence showed that our approach outperformed all the baselines in the audio identification task and other classification tasks related to the attributes of the audio signal with an economical hash size of either 128 or 256 bits  for one second of audio.     Additionally, the developed technology was deployed into two 9-1-1 Emergency Operation Centers (EOCs), located in Palm Beach County (PBC) and Greater Harris County (GH), allowing us to evaluate the performance in real-time in an industrial  environment.},
  copyright = {openAccess},
  language = {eng},
  annotation = {Accepted: 2020-04-17T16:43:34Z},
  file = {/Users/pasinduwijesena/Zotero/storage/6E9HXEWH/10.1145_3380828_SAMAF.pdf;/Users/pasinduwijesena/Zotero/storage/MDZT6K3X/MDZT6K3X.pdf;/Users/pasinduwijesena/Zotero/storage/J6GC5V2M/636319.html}
}

@misc{BMATReportal,
  title = {{{BMAT}} - {{Reportal}}},
  journal = {BMAT},
  abstract = {Automating TV music reporting for broadcasters.},
  howpublished = {https://www.bmat.com/reportal/},
  language = {en-US}
}

@book{book,
  title = {Fundamentals of {{Music Processing}}: {{Audio}}, {{Analysis}}, {{Algorithms}}, {{Applications}}},
  shorttitle = {Fundamentals of {{Music Processing}}},
  author = {M{\"u}ller, Meinard},
  year = {2015},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-21945-5},
  abstract = {This textbook provides both profound technological knowledge and a comprehensive treatment of essential topics in music processing and music information retrieval. Including numerous examples, figures, and exercises, this book is suited for students, lecturers, and researchers working in audio engineering, computer science, multimedia, and musicology.The book consists of eight chapters. The first two cover foundations of music representations and the Fourier transform\textemdash concepts that are then used throughout the book. In the subsequent chapters, concrete music processing tasks serve as a starting point. Each of these chapters is organized in a similar fashion and starts with a general description of the music processing scenario at hand before integrating it into a wider context. It then discusses\textemdash in a mathematically rigorous way\textemdash important techniques and algorithms that are generally applicable to a wide range of analysis, classification, and retrieval problems. At the same time, the techniques are directly applied to a specific music processing task. By mixing theory and practice, the book's goal is to offer detailed technological insights as well as a deep understanding of music processing applications. Each chapter ends with a section that includes links to the research literature, suggestions for further reading, a list of references, and exercises. The chapters are organized in a modular fashion, thus offering lecturers and readers many ways to choose, rearrange or supplement the material. Accordingly, selected chapters or individual sections can easily be integrated into courses on general multimedia, information science, signal processing, music informatics, or the digital humanities.},
  isbn = {978-3-319-21944-8},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/K85QRX7E/Meinard - Fundamentals of Music Processing Audio, Analysis,.pdf;/Users/pasinduwijesena/Zotero/storage/UIKLMETA/MÃ¼ller - 2015 - Fundamentals of Music Processing Audio, Analysis,.pdf;/Users/pasinduwijesena/Zotero/storage/T7Z8KYZK/9783319219448.html}
}

@article{boonDynamicalSystemsTheory1995,
  title = {Dynamical Systems Theory for Music Dynamics},
  author = {Boon, Jean Pierre and Decroly, Olivier},
  year = {1995},
  month = sep,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {5},
  number = {3},
  eprint = {chao-dyn/9411022},
  eprinttype = {arxiv},
  pages = {501--508},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.166145},
  abstract = {We show that, when music pieces are cast in the form of time series of pitch variations, the concepts and tools of dynamical systems theory can be applied to the analysis of \{\textbackslash it temporal dynamics\} in music. (i) Phase space portraits are constructed from the time series wherefrom the dimensionality is evaluated as a measure of the \{\textbackslash pit global\} dynamics of each piece. (ii) Spectral analysis of the time series yields power spectra (\$\textbackslash sim f\^\{-\textbackslash nu\}\$) close to \{\textbackslash pit red noise\} (\$\textbackslash nu \textbackslash sim 2\$) in the low frequency range. (iii) We define an information entropy which provides a measure of the \{\textbackslash pit local\} dynamics in the musical piece; the entropy can be interpreted as an evaluation of the degree of \{\textbackslash it complexity\} in the music, but there is no evidence of an analytical relation between local and global dynamics. These findings are based on computations performed on eighty sequences sampled in the music literature from the 18th to the 20th century.},
  archiveprefix = {arXiv},
  keywords = {Nonlinear Sciences - Chaotic Dynamics},
  file = {/Users/pasinduwijesena/Zotero/storage/8N44EYR4/Boon and Decroly - 1995 - Dynamical systems theory for music dynamics.pdf;/Users/pasinduwijesena/Zotero/storage/DDWEUPXI/9411022.html}
}

@misc{brennerPowerRecurrentNeural2019,
  title = {The {{Power}} of {{Recurrent Neural Networks}}},
  author = {Brenner, Manuel},
  year = {2019},
  month = dec,
  journal = {Medium},
  abstract = {\ldots and how they allow us to learn almost any dynamical system given the right  data},
  howpublished = {https://towardsdatascience.com/the-power-of-recurrent-neural-networks-1ef056dae2a5},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/JDU7KHLG/the-power-of-recurrent-neural-networks-1ef056dae2a5.html}
}

@article{burrowsDynamicalSystemsPerspective1997,
  title = {A {{Dynamical Systems Perspective}} on {{Music}}},
  author = {Burrows, David},
  year = {1997},
  month = oct,
  journal = {Journal of Musicology},
  volume = {15},
  number = {4},
  pages = {529--545},
  issn = {0277-9269},
  doi = {10.2307/764006},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/ZHU862VA/Burrows - 1997 - A Dynamical Systems Perspective on Music.pdf}
}

@inproceedings{canoContentbasedMusicAudio2005,
  title = {Content-Based Music Audio Recommendation},
  author = {Cano, Pedro and Koppenberger, Markus and Wack, Nicolas},
  year = {2005},
  month = jan,
  pages = {211--212},
  doi = {10.1145/1101149.1101181},
  abstract = {We present the MusicSurfer, a metadata free system for the interaction with massive collections of music. MusicSurfer automatically extracts descriptions related to instrumentation, rhythm and harmony from music audio signals. Together with efficient similarity metrics, the descriptions allow navigation of multimillion track music collections in a flexible and efficient way without the need for metadata nor human ratings.},
  file = {/Users/pasinduwijesena/Zotero/storage/3KDAX2QZ/Cano et al. - 2005 - Content-based music audio recommendation.pdf}
}

@article{canoReviewAudioFingerprinting2005,
  title = {A {{Review}} of {{Audio Fingerprinting}}},
  author = {Cano, Pedro and Batlle, Eloi and Kalker, Ton and Haitsma, Jaap},
  year = {2005},
  month = nov,
  journal = {Journal of VLSI signal processing systems for signal, image and video technology},
  volume = {41},
  number = {3},
  pages = {271--284},
  issn = {0922-5773},
  doi = {10.1007/s11265-005-4151-3},
  abstract = {An audio fingerprint is a compact content-based signature that summarizes an audio recording. Audio Fingerprinting technologies have attracted attention since they allow the identification of audio independently of its format and without the need of meta-data or watermark embedding. Other uses of fingerprinting include: integrity verification, watermark support and content-based audio retrieval. The different approaches to fingerprinting have been described with different rationales and terminology: Pattern matching, Multimedia (Music) Information Retrieval or Cryptography (Robust Hashing). In this paper, we review different techniques describing its functional blocks as parts of a common, unified framework.},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/LXIWSD8K/Cano et al. - 2005 - A Review of Audio Fingerprinting.pdf}
}

@article{chathurangaAutomaticMusicGenre2013,
  title = {Automatic {{Music Genre Classification}} of {{Audio Signals}} with {{Machine Learning Approaches}}},
  author = {Chathuranga, Devindu and Jayaratne, Lakshman},
  year = {2013},
  month = aug,
  journal = {GSTF Journal on Computing (JoC)},
  volume = {3},
  doi = {10.7603/s40601-013-0014-0},
  abstract = {Musical genre classification is put into context by explaining about the structures in music and how it is analyzed and perceived by humans. The increase of the music databases on the personal collection and the Internet has brought a great demand for music information retrieval, and especially automatic musical genre classification. In this research we focused on combining information from the audio signal than different sources. This paper presents a comprehensive machine learning approach to the problem of automatic musical genre classification using the audio signal. The proposed approach uses two feature vectors, Support vector machine classifier with polynomial kernel function and machine learning algorithms. More specifically, two feature sets for representing frequency domain, temporal domain, cepstral domain and modulation frequency domain audio features are proposed. Using our proposed features SVM act as strong base learner in AdaBoost, so its performance of the SVM classifier cannot improve using boosting method. The final genre classification is obtained from the set of individual results according to a weighting combination late fusion method and it outperformed the trained fusion method. Music genre classification accuracy of 78\% and 81\% is reported on the GTZAN dataset over the ten musical genres and the ISMIR2004 genre dataset over the six musical genres, respectively. We observed higher classification accuracies with the ensembles, than with the individual classifiers and improvements of the performances on the GTZAN and ISMIR2004 genre datasets are three percent on average. This ensemble approach show that it is possible to improve the classification accuracy by using different types of domain based audio features.},
  file = {/Users/pasinduwijesena/Zotero/storage/S8ZPCFE7/S8ZPCFE7.pdf}
}

@article{chenLearningAudioEmbeddings2020,
  title = {Learning {{Audio Embeddings}} with {{User Listening Data}} for {{Content}}-Based {{Music Recommendation}}},
  author = {Chen, Ke and Liang, Beici and Ma, Xiaoshuan and Gu, Minwei},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.15389 [cs, eess]},
  eprint = {2010.15389},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Personalized recommendation on new track releases has always been a challenging problem in the music industry. To combat this problem, we first explore user listening history and demographics to construct a user embedding representing the user's music preference. With the user embedding and audio data from user's liked and disliked tracks, an audio embedding can be obtained for each track using metric learning with Siamese networks. For a new track, we can decide the best group of users to recommend by computing the similarity between the track's audio embedding and different user embeddings, respectively. The proposed system yields state-of-the-art performance on content-based music recommendation tested with millions of users and tracks. Also, we extract audio embeddings as features for music genre classification tasks. The results show the generalization ability of our audio embeddings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/pasinduwijesena/Zotero/storage/G7DG5UJP/Chen et al. - 2020 - Learning Audio Embeddings with User Listening Data.pdf;/Users/pasinduwijesena/Zotero/storage/8PMVXX55/2010.html}
}

@book{chiuBackgroundMusicIdentification2010,
  title = {Background Music Identification through Content Filtering and Min-Hash Matching},
  author = {Chiu, Chih-Yi and Bountouridis, Dimitrios and Wang, Ju-Chiang and Wang, Hsin-min},
  year = {2010},
  month = mar,
  journal = {Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference on},
  pages = {2417},
  doi = {10.1109/ICASSP.2010.5496255},
  abstract = {A novel framework for background music identification is proposed in this paper. Given a piece of audio signals that mixes background music with speech/noise, we identify the music part with source music data. Conventional methods that take the whole audio signals for identification are inap- propriate in terms of efficiency and accuracy. In our frame- work, the audio content is filtered through speech center cancellation and noise removal to extract clear music seg- ments. To identify these music segments, we use a compact feature representation and efficient similarity measurement based on the min-hash theory. The results of experiments on the RWC music database show a promising direction. To alleviate the above problems, we propose a novel framework for background music identification. Two prac- tical steps are employed to analyze the audio signal. First, based on our observation in TV broadcasting and video, we leverage the stereo format capability to cancel the speech that is panned to the center. Second, for noise and speech that cannot be cancelled, they are located through GMM- based classification and removed to obtain clear music snip- pets for identification. Thus, we can reduce unnecessary comparisons and false positives. To achieve efficient and accurate identification, we present a min-hash-based feature representation and similar- ity measurement. Each music frame is represented by a one- dimensional min-hash value, and the similarity between two music segments is computed by their hash collision rate. Fig. 1 shows an overview of the proposed framework. We vali- date the framework through several experiments performed on the RWC music database (5). The remainder of this paper is organized as follows. Sections 2 and 3 describe the content filtering and min-hash indexing, respectively. Sections 4 and 5 summarize the ex- periment results and conclusions, respectively.}
}

@misc{ClassicalPianoMidi,
  title = {Classical {{Piano Midi Page}} - {{Main Page}}},
  howpublished = {http://www.piano-midi.de/},
  file = {/Users/pasinduwijesena/Zotero/storage/JLUWHC7M/www.piano-midi.de.html}
}

@article{complex_dynamics,
  title = {Complex Dynamics and Musical Structure},
  author = {Boon, Jean-Pierre and Noullez, Alain and Mommen, Corinne},
  year = {1990},
  month = jan,
  journal = {Interface},
  volume = {19},
  number = {1},
  pages = {3--14},
  publisher = {{Routledge}},
  issn = {0303-3902},
  doi = {10.1080/09298219008570553},
  abstract = {A piece of music is generally perceived as the time evolution of acoustical signals, but, because of its complexity, the intrinsic dynamics of the music cannot be easily identified or characterized. A musical sequence can also be considered as the time series resulting from a dynamical phenomenon. We show that the theory of dynamic systems provides interesting tools for the identification of complex dynamics in music.},
  annotation = {\_eprint: https://doi.org/10.1080/09298219008570553},
  file = {/Users/pasinduwijesena/Zotero/storage/2VL7NRH2/Boon et al. - 1990 - Complex dynamics and musical structure.pdf;/Users/pasinduwijesena/Zotero/storage/4URV3GWV/09298219008570553.html}
}

@inproceedings{computer_vision_for_music_identification,
  title = {Computer {{Vision}} for {{Music Identification}}},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  author = {{Yan Ke} and Hoiem, D. and Sukthankar, R.},
  year = {2005},
  volume = {1},
  pages = {597--604},
  publisher = {{IEEE}},
  address = {{San Diego, CA, USA}},
  doi = {10.1109/CVPR.2005.105},
  abstract = {We describe how certain tasks in the audio domain can be effectively addressed using computer vision approaches. This paper focuses on the problem of music identification, where the goal is to reliably identify a song given a few seconds of noisy audio. Our approach treats the spectrogram of each music clip as a 2-D image and transforms music identification into a corrupted sub-image retrieval problem. By employing pairwise boosting on a large set of Viola-Jones features, our system learns compact, discriminative, local descriptors that are amenable to efficient indexing. During the query phase, we retrieve the set of song snippets that locally match the noisy sample and employ geometric verification in conjunction with an EM-based ``occlusion'' model to identify the song that is most consistent with the observed signal. We have implemented our algorithm in a practical system that can quickly and accurately recognize music from short audio samples in the presence of distortions such as poor recording quality and significant ambient noise. Our experiments demonstrate that this approach significantly outperforms the current state-of-theart in content-based music identification.},
  isbn = {978-0-7695-2372-9},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/BMHAN84N/Yan Ke et al. - 2005 - Computer Vision for Music Identification.pdf;/Users/pasinduwijesena/Zotero/storage/CMTQU3GT/1467322.html}
}

@article{ConstantQTransform2020,
  title = {Constant-{{Q}} Transform},
  year = {2020},
  month = oct,
  journal = {Wikipedia},
  abstract = {In mathematics and signal processing, the constant-Q transform transforms a data series to the frequency domain. It is related to the Fourier transform and very closely related to the complex Morlet wavelet transform.The transform can be thought of as a series of filters fk, logarithmically spaced in frequency, with the k-th filter having a spectral width {$\delta$}fk equal to a multiple of the previous filter's width:                                                                                {$\delta$}                                    f                                        k                                                                                                    =                                    2                                        1                                            /                                          n                                                     {$\cdot$}                 {$\delta$}                                    f                                        k                     -                     1                                                                                                                                             =                                                         (                                            2                                                1                                                    /                                                  n                                                                 )                                                           k                                                     {$\cdot$}                 {$\delta$}                                    f                                        min                                                     ,                                                                 \{\textbackslash displaystyle \{\textbackslash begin\{aligned\}\textbackslash delta f\_\{k\}\&=2\^\{1/n\}\textbackslash cdot \textbackslash delta f\_\{k-1\}\textbackslash\textbackslash\&=\textbackslash left(2\^\{1/n\}\textbackslash right)\^\{k\}\textbackslash cdot \textbackslash delta f\_\{\textbackslash text\{min\}\},\textbackslash end\{aligned\}\}\}   where {$\delta$}fk is the bandwidth of the k-th filter, fmin is the central frequency of the lowest filter, and n is the number of filters per octave.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  language = {en},
  annotation = {Page Version ID: 981501242},
  file = {/Users/pasinduwijesena/Zotero/storage/RHG5R8CH/index.html}
}

@misc{Covers80CoverSong,
  title = {The Covers80 Cover Song Data Set},
  howpublished = {http://labrosa.ee.columbia.edu/projects/coversongs/covers80/},
  file = {/Users/pasinduwijesena/Zotero/storage/JYY67P6S/covers80.html}
}

@misc{CS230Deep,
  title = {{{CS}} 230 - {{Deep Learning Tips}} and {{Tricks Cheatsheet}}},
  howpublished = {https://stanford.edu/\textasciitilde shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks},
  file = {/Users/pasinduwijesena/Zotero/storage/99LBRM98/cheatsheet-deep-learning-tips-and-tricks.html}
}

@misc{CS230Deepa,
  title = {{{CS}} 230 - {{Deep Learning Tips}} and {{Tricks Cheatsheet}}},
  howpublished = {https://stanford.edu/\textasciitilde shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks\#running-nn},
  file = {/Users/pasinduwijesena/Zotero/storage/YK3K73D4/cheatsheet-deep-learning-tips-and-tricks.html}
}

@misc{Datasets2021,
  title = {Datasets},
  year = {2021},
  month = aug,
  journal = {ISMIR},
  abstract = {Resources / Datasets},
  howpublished = {https://ismir.net/resources/datasets/},
  language = {en-US},
  file = {/Users/pasinduwijesena/Zotero/storage/H9DU3LUM/datasets.html}
}

@misc{DatasetsISMIR,
  title = {Datasets | {{ISMIR}}},
  howpublished = {https://ismir.net/resources/datasets/}
}

@misc{DeezerSpleeter2020,
  title = {Deezer/Spleeter},
  year = {2020},
  month = nov,
  abstract = {Deezer source separation library including pretrained models.},
  copyright = {MIT License         ,                 MIT License},
  howpublished = {Deezer},
  keywords = {audio-processing,bass,deep-learning,deezer,drums,model,pretrained-models,python,tensorflow,vocals}
}

@article{defferrardFMADatasetMusic2017,
  title = {{{FMA}}: {{A Dataset For Music Analysis}}},
  shorttitle = {{{FMA}}},
  author = {Defferrard, Micha{\"e}l and Benzi, Kirell and Vandergheynst, Pierre and Bresson, Xavier},
  year = {2017},
  month = sep,
  journal = {arXiv:1612.01840 [cs]},
  eprint = {1612.01840},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections. The community's growing interest in feature and end-to-end learning is however restrained by the limited availability of large audio datasets. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies. We here describe the dataset and how it was created, propose a train/validation/test split and three subsets, discuss some suitable MIR tasks, and evaluate some baselines for genre recognition. Code, data, and usage examples are available at https://github.com/mdeff/fma},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Sound},
  file = {/Users/pasinduwijesena/Zotero/storage/UPJYNB6H/Defferrard et al. - 2017 - FMA A Dataset For Music Analysis.pdf;/Users/pasinduwijesena/Zotero/storage/UE5LMTDA/1612.html}
}

@article{defferrardFMADatasetMusic2017a,
  title = {{{FMA}}: {{A Dataset For Music Analysis}}},
  shorttitle = {{{FMA}}},
  author = {Defferrard, Micha{\"e}l and Benzi, Kirell and Vandergheynst, Pierre and Bresson, Xavier},
  year = {2017},
  month = sep,
  journal = {arXiv:1612.01840 [cs]},
  eprint = {1612.01840},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections. The community's growing interest in feature and end-to-end learning is however restrained by the limited availability of large audio datasets. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies. We here describe the dataset and how it was created, propose a train/validation/test split and three subsets, discuss some suitable MIR tasks, and evaluate some baselines for genre recognition. Code, data, and usage examples are available at https://github.com/mdeff/fma},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Sound},
  file = {/Users/pasinduwijesena/Zotero/storage/GF35HJ4D/Defferrard et al. - 2017 - FMA A Dataset For Music Analysis.pdf;/Users/pasinduwijesena/Zotero/storage/R4W52WZ2/1612.html}
}

@misc{doAudioFingerprintingHow2018,
  title = {Audio {{Fingerprinting}} \textendash{} {{How We Identify Songs}}},
  author = {Do, That Thing We},
  year = {2018},
  month = mar,
  journal = {That Thing We Do},
  abstract = {Audio fingerprinting is literally our identity sign, the system that allows us to identify 52 years of audio against 60M sound recordings every day.},
  language = {en-GB},
  file = {/Users/pasinduwijesena/Zotero/storage/4G58SRN6/how-we-identify-songs-audio-fingerprinting.html}
}

@article{dorasCoverDetectionUsing2019,
  title = {Cover {{Detection}} Using {{Dominant Melody Embeddings}}},
  author = {Doras, Guillaume and Peeters, Geoffroy},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.01824 [cs, stat]},
  eprint = {1907.01824},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Automatic cover detection -- the task of finding in an audio database all the covers of one or several query tracks -- has long been seen as a challenging theoretical problem in the MIR community and as an acute practical problem for authors and composers societies. Original algorithms proposed for this task have proven their accuracy on small datasets, but are unable to scale up to modern real-life audio corpora. On the other hand, faster approaches designed to process thousands of pairwise comparisons resulted in lower accuracy, making them unsuitable for practical use. In this work, we propose a neural network architecture that is trained to represent each track as a single embedding vector. The computation burden is therefore left to the embedding extraction -- that can be conducted offline and stored, while the pairwise comparison task reduces to a simple Euclidean distance computation. We further propose to extract each track's embedding out of its dominant melody representation, obtained by another neural network trained for this task. We then show that this architecture improves state-of-the-art accuracy both on small and large datasets, and is able to scale to query databases of thousands of tracks in a few seconds.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Statistics - Machine Learning},
  file = {/Users/pasinduwijesena/Zotero/storage/D9LWNAW2/Doras and Peeters - 2019 - Cover Detection using Dominant Melody Embeddings.pdf;/Users/pasinduwijesena/Zotero/storage/QT8GBTRG/1907.html}
}

@article{dynamic_process_modeling_with_rnn,
  title = {Dynamic Process Modeling with Recurrent Neural Networks},
  author = {You, Yong and Nikolaou, Michael},
  year = {1993},
  journal = {AIChE Journal},
  volume = {39},
  number = {10},
  pages = {1654--1667},
  issn = {1547-5905},
  doi = {10.1002/aic.690391009},
  abstract = {A method of nonhlinear static and dynamic process modeling via recurrent neural networks (RNNs) is studied. An RNN model is a set of coupled nonlinear ordinary differential equations in continuous time domain with nonlinear dynamic node characteristics as well as both feedforward and feedback connections. For such networks, each physical input to a system corresponds to exactly one input to the network. The system's dynamics are captured by the internal structure of the network. The structure of RNN models may be more natural and attractive than that of feedforward neural network models, but computation time for training is longer. Our simulation results show that RNNs can learn both steady-state relationships and process dynamics of continuous and batch, single-input/single-output and multiinput/multioutput systems in a simple and direct manner. Training of RNNs shows only small degradation in the presence of noise in the training data. Thus, RNNs constitute a feasible alternative to layered feedforward back propagation neural networks in steady-state and dynamic process modeling and model-based control.},
  copyright = {Copyright \textcopyright{} 1993 American Institute of Chemical Engineers},
  language = {en},
  annotation = {\_eprint: https://aiche.onlinelibrary.wiley.com/doi/pdf/10.1002/aic.690391009},
  file = {/Users/pasinduwijesena/Zotero/storage/LT4HBMJV/You and Nikolaou - 1993 - Dynamic process modeling with recurrent neural net.pdf;/Users/pasinduwijesena/Zotero/storage/HF2XYLQI/aic.html}
}

@misc{ellisEchoPrintOpenSource2011,
  title = {{{EchoPrint}} - {{An Open Source Music Identification Service}}},
  author = {Ellis, Daniel P.W. and Whitman, Brian and Porter, Alastair},
  year = {2011},
  abstract = {We discuss Echoprint, an open source and open data music identification service available to anyone. Echoprint is efficient and speedy and works by generating dozens of hashes a second from input audio (microphone or files) and then matching those hashes in a large scale inverted index for queries. We discuss the signal code generator and the server component. 1},
  file = {/Users/pasinduwijesena/Zotero/storage/CB2EWAXG/Ellis et al. - 2011 - EchoPrint - An Open Source Music Identification Se.pdf}
}

@misc{FiftyChallengingProblems,
  title = {Fifty {{Challenging Problems}} in {{Probability}}},
  howpublished = {http://www2.washjeff.edu/users/mwoltermann/Mosteller/contents.htm},
  file = {/Users/pasinduwijesena/Zotero/storage/8AUZPGSM/contents.html}
}

@misc{FinaleGoogleDrive,
  title = {Finale - {{Google Drive}}},
  howpublished = {https://drive.google.com/drive/folders/1QM7h0I7RXko65RdYJUB7UhFfIl\_eOLdT},
  file = {/Users/pasinduwijesena/Zotero/storage/E4IBL2J5/Finale - Google Drive.pdf;/Users/pasinduwijesena/Zotero/storage/4VHLEBHC/1QM7h0I7RXko65RdYJUB7UhFfIl_eOLdT.html}
}

@article{footeARTHURRetrievingOrchestral,
  title = {{{ARTHUR}}: {{Retrieving Orchestral Music}} by {{Long}}-{{Term Structure}}},
  author = {Foote, Jonathan},
  pages = {6},
  abstract = {We introduce an audio retrieval-by-example system for orchestral music. Unlike many other approaches, this system is based on analysis of the audio waveform and does not rely on symbolic or MIDI representations. ARTHUR retrieves audio on the basis of long-term structure, specifically the variation of soft and louder passages. The longterm structure is determined from envelope of audio energy versus time in one or more frequency bands. Similarity between energy profiles is calculated using dynamic programming. Given an example audio document, other documents in a collection can be ranked by similarity of their energy profiles. Experiments are presented for a modest corpus that demonstrate excellent results in retrieving different performances of the same orchestral work, given an example performance or short excerpt as a query.},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/VD5NTRTV/Foote - ARTHUR Retrieving Orchestral Music by Long-Term S.pdf}
}

@article{gersLearningForgetContinual2000,
  title = {Learning to {{Forget}}: {{Continual Prediction}} with {{LSTM}}},
  shorttitle = {Learning to {{Forget}}},
  author = {Gers, Felix and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  year = {2000},
  month = oct,
  journal = {Neural computation},
  volume = {12},
  pages = {2451--71},
  doi = {10.1162/089976600300015015},
  abstract = {Long short-term memory (LSTM; Hochreiter \& Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive "forget gate" that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way.},
  file = {/Users/pasinduwijesena/Zotero/storage/4B7ZBP5A/Gers et al. - 2000 - Learning to Forget Continual Prediction with LSTM.pdf}
}

@inproceedings{gomezSongRemainsSame2006,
  title = {The Song Remains the Same: {{Identifying}} Versions of the Same Piece Using Tonal Descriptors},
  shorttitle = {The Song Remains the Same},
  booktitle = {University of {{Victoria}}},
  author = {G{\'o}mez, Emilia},
  year = {2006},
  pages = {180--185},
  abstract = {Identifying versions of the same song by means of automatically extracted audio features is a complex task for a music information retrieval system, even though it may seem very simple for a human listener. The design of a system to perform this task gives the opportunity to analyze which features are relevant for music similarity. This paper focuses on the analysis of tonal similarity and its application to the identification of different versions of the same piece. This work formulates the situations where a song is versioned and several musical aspects are transformed with respect to the canonical version. A quantitative evaluation is made using tonal descriptors, including chroma representations and tonality. A simple similarity measure, based on Dynamic Time Warping over transposed chroma features, yields around 55 \% accuracy, which exceeds by far the expected random baseline rate.},
  file = {/Users/pasinduwijesena/Zotero/storage/M5E4TQFH/GÃ³mez - 2006 - The song remains the same Identifying versions of.pdf;/Users/pasinduwijesena/Zotero/storage/XRIAA6Y2/download.html}
}

@article{groscheAudioContentBasedMusic2012,
  title = {Audio {{Content}}-{{Based Music Retrieval}}},
  author = {Grosche, Peter and M{\"u}ller, Meinard and Serr{\`a}, Joan},
  year = {2012},
  pages = {18 pages},
  publisher = {{Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany}},
  doi = {10.4230/DFU.VOL3.11041.157},
  abstract = {The rapidly growing corpus of digital audio material requires novel retrieval strategies for exploring large music collections. Traditional retrieval strategies rely on metadata that describe the actual audio content in words. In the case that such textual descriptions are not available, one requires content-based retrieval strategies which only utilize the raw audio material. In this contribution, we discuss content-based retrieval strategies that follow the query-by-example paradigm: given an audio query, the task is to retrieve all documents that are somehow similar or related to the query from a music collection. Such strategies can be loosely classified according to their specificity, which refers to the degree of similarity between the query and the database documents. Here, high specificity refers to a strict notion of similarity, whereas low specificity to a rather vague one. Furthermore, we introduce a second classification principle based on granularity, where one distinguishes between fragment-level and document-level retrieval. Using a classification scheme based on specificity and granularity, we identify various classes of retrieval scenarios, which comprise audio identification, audio matching, and version identification. For these three important classes, we give an overview of representative state-of-the-art approaches, which also illustrate the sometimes subtle but crucial differences between the retrieval scenarios. Finally, we give an outlook on a user-oriented retrieval system, which combines the various retrieval strategies in a unified framework.},
  collaborator = {Herbstritt, Marc},
  language = {en},
  keywords = {000 Computer science; knowledge; general works,Computer Science},
  file = {/Users/pasinduwijesena/Zotero/storage/2H8NE9D2/Grosche et al. - 2012 - Audio Content-Based Music Retrieval.pdf}
}

@article{haitsmaHighlyRobustAudio,
  title = {A {{Highly Robust Audio Fingerprinting System}}},
  author = {Haitsma, Jaap and Kalker, Ton},
  pages = {9},
  abstract = {Imagine the following situation. You're in your car, listening to the radio and suddenly you hear a song that catches your attention. It's the best new song you have heard for a long time, but you missed the announcement and don't recognize the artist. Still, you would like to know more about this music. What should you do? You could call the radio station, but that's too cumbersome. Wouldn't it be nice if you could push a few buttons on your mobile phone and a few seconds later the phone would respond with the name of the artist and the title of the music you're listening to? Perhaps even sending an email to your default email address with some supplemental information. In this paper we present an audio fingerprinting system, which makes the above scenario possible. By using the fingerprint of an unknown audio clip as a query on a fingerprint database, which contains the fingerprints of a large library of songs, the audio clip can be identified. At the core of the presented system are a highly robust fingerprint extraction method and a very efficient fingerprint search strategy, which enables searching a large fingerprint database with only limited computing resources.},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/557B6NVF/Haitsma and Kalker - A Highly Robust Audio Fingerprinting System.pdf}
}

@article{huzaifahComparisonTimeFrequencyRepresentations2017,
  title = {Comparison of {{Time}}-{{Frequency Representations}} for {{Environmental Sound Classification}} Using {{Convolutional Neural Networks}}},
  author = {Huzaifah, M.},
  year = {2017},
  month = jun,
  journal = {arXiv:1706.07156 [cs]},
  eprint = {1706.07156},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent successful applications of convolutional neural networks (CNNs) to audio classification and speech recognition have motivated the search for better input representations for more efficient training. Visual displays of an audio signal, through various time-frequency representations such as spectrograms offer a rich representation of the temporal and spectral structure of the original signal. In this letter, we compare various popular signal processing methods to obtain this representation, such as short-time Fourier transform (STFT) with linear and Mel scales, constant-Q transform (CQT) and continuous Wavelet transform (CWT), and assess their impact on the classification performance of two environmental sound datasets using CNNs. This study supports the hypothesis that time-frequency representations are valuable in learning useful features for sound classification. Moreover, the actual transformation used is shown to impact the classification accuracy, with Mel-scaled STFT outperforming the other discussed methods slightly and baseline MFCC features to a large degree. Additionally, we observe that the optimal window size during transformation is dependent on the characteristics of the audio signal and architecturally, 2D convolution yielded better results in most cases compared to 1D.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/pasinduwijesena/Zotero/storage/BU345ZYW/Huzaifah - 2017 - Comparison of Time-Frequency Representations for E.pdf;/Users/pasinduwijesena/Zotero/storage/H4KHJ672/1706.html}
}

@book{jayaratneSupervisedLearningApproach2016,
  title = {Supervised {{Learning Approach}} for {{Classification}} of {{Sri Lankan Music}} Based on {{Music Structure Similarity}}},
  author = {Jayaratne, Lakshman and Peiris, Rajitha},
  year = {2016},
  month = mar,
  doi = {10.5176/2251-1679_CGAT16.25}
}

@inproceedings{jiang_yang_chen_2020,
  title = {Similarity {{Learning For Cover Song Identification Using Cross}}-{{Similarity Matrices}} of {{Multi}}-{{Level Deep Sequences}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Jiang, Chaoya and Yang, Deshun and Chen, Xiaoou},
  year = {2020},
  month = may,
  pages = {26--30},
  publisher = {{IEEE}},
  address = {{Barcelona, Spain}},
  doi = {10.1109/ICASSP40776.2020.9053257},
  abstract = {In recent years, several deep learning models have been proposed for cover song identification and they have been designed to learn fixed-length feature vectors for music tracks. However, the aspect of temporal progression of music, which is important for measuring the melody similarity between two tracks, is not well represented by fixed-length vectors. In this paper, we propose a new Siamese network architecture for music melody similarity metric learning. The architecture consists of two parts. One part is a network for learning the deep sequence representation of music tracks, and the other is a similarity estimation network which takes as input the crosssimilarity matrices calculated from the deep sequences of a pair of tracks. The two networks are jointly trained and optimized to achieve high melody similarity prediction accuracy. Experiments conducted on several public datasets demonstrate the superiority of the proposed architecture.},
  isbn = {978-1-5090-6631-5},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/IGEJG28T/Jiang et al. - 2020 - Similarity Learning For Cover Song Identification .pdf}
}

@article{jrEndtoEndMusicTranscription,
  title = {End-to-{{End Music Transcription Using Fine}}-{{Tuned Variable}}-{{Q Filterbanks}}},
  author = {Jr, Frank C Cwitkowitz},
  pages = {68},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/U6IHSQTT/Jr - End-to-End Music Transcription Using Fine-Tuned Va.pdf}
}

@article{karsdorpLEARNINGSIMILARITYMETRICS2019,
  title = {{{LEARNING SIMILARITY METRICS FOR MELODY RETRIEVAL}}},
  author = {Karsdorp, Folgert and {van Kranenburg}, Peter and Manjavacas, Enrique},
  year = {2019},
  pages = {8},
  abstract = {Similarity measures are indispensable in music information retrieval. In recent years, various proposals have been made for measuring melodic similarity in symbolically encoded scores. Many of these approaches are ultimately based on a dynamic programming approach such as sequence alignment or edit distance, which has various drawbacks. First, the similarity scores are not necessarily metrics and are not directly comparable. Second, the algorithms are mostly first-order and of quadratic timecomplexity, and finally, the features and weights need to be defined precisely. We propose an alternative approach which employs deep neural networks for end-to-end similarity metric learning. We contrast and compare different recurrent neural architectures (LSTM and GRU) for representing symbolic melodies as continuous vectors, and demonstrate how duplet and triplet loss functions can be employed to learn compact distributional representations of symbolic music in an induced melody space. This approach is contrasted with an alignment-based approach. We present results for the Meertens Tune Collections, which consists of a large number of vocal and instrumental monophonic pieces from Dutch musical sources, spanning five centuries, and demonstrate the robustness of the learned similarity metrics.},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/36YUAT3V/Karsdorp et al. - 2019 - LEARNING SIMILARITY METRICS FOR MELODY RETRIEVAL.pdf}
}

@article{kimuraVeryQuickAudio2003,
  title = {Very Quick Audio Searching: {{Introducing}} Global Pruning to the {{Time}}-{{Series Active Search}}},
  shorttitle = {Very Quick Audio Searching},
  author = {Kimura, Akisato and Kashino, Kunio and Kurozumi, Takayuki and Murase, H.},
  year = {2003},
  month = may,
  journal = {Acoustics, Speech, and Signal Processing, IEEE International Conference on},
  volume = {3},
  pages = {1429--1432},
  issn = {0-7803-7041-4},
  doi = {10.1109/ICASSP.2001.941198},
  abstract = {Previously, we proposed a histogram-based quick signal search method called Time-Series Active Search (TAS). TAS is a method of searching through long audio or video recordings for a specified segment, based on signal similarity. TAS is fast; it can search through a 24-hour recording in 1 second after a query-independent preprocessing. However, an even faster method is required when we consider a huge amount of audio archives, for example a month's worth of recordings. Thus, we propose a preprocessing method that significantly accelerates TAS. The core part of this method comprises a global histogram clustering of long signals and a pruning scheme using those clusters. Tests using broadcast recording indicate that the proposed algorithm achieves a search speed approximately 3 to 30 times faster than TAS. In these tests, the search results are exactly the same as with TAS.},
  file = {/Users/pasinduwijesena/Zotero/storage/IQAXVM4N/Kimura et al. - 2003 - Very quick audio searching Introducing global pru.pdf}
}

@article{klimontovichNaturalFlickerNoise1987,
  title = {Natural {{Flicker Noise}} (``1/f {{Noise}}'') in {{Music}}},
  author = {Klimontovich, Yu and Boon, Jean},
  year = {1987},
  month = feb,
  journal = {Europhysics Letters (epl)},
  volume = {3},
  pages = {395--399},
  doi = {10.1209/0295-5075/3/4/002},
  abstract = {A large class of musical selections exhibits a spectral density of audio power fluctuations characterized by a low-frequency behaviour typical of 1/f noise. We show that this 1/f behaviour follows from natural flicker noise theory.},
  file = {/Users/pasinduwijesena/Zotero/storage/4L3FUUQF/Klimontovich and Boon - 1987 - Natural Flicker Noise (â1f Noiseâ) in Music.pdf}
}

@article{kostadinovNetworkParameterLearning2019,
  title = {Network {{Parameter Learning Using Nonlinear Transforms}}, {{Local Representation Goals}} and {{Local Propagation Constraints}}},
  author = {Kostadinov, Dimche and Razdehi, Behrooz and Voloshynovskiy, Slava},
  year = {2019},
  month = jan,
  journal = {arXiv:1902.00016 [cs, stat]},
  eprint = {1902.00016},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this paper, we introduce a novel concept for learning of the parameters in a neural network. Our idea is grounded on modeling a learning problem that addresses a trade-off between (i) satisfying local objectives at each node and (ii) achieving desired data propagation through the network under (iii) local propagation constraints. We consider two types of nonlinear transforms which describe the network representations. One of the nonlinear transforms serves as activation function. The other one enables a locally adjusted, deviation corrective components to be included in the update of the network weights in order to enable attaining target specific representations at the last network node. Our learning principle not only provides insight into the understanding and the interpretation of the learning dynamics, but it offers theoretical guarantees over decoupled and parallel parameter estimation strategy that enables learning in synchronous and asynchronous mode. Numerical experiments validate the potential of our approach on image recognition task. The preliminary results show advantages in comparison to the state-of-the-art methods, w.r.t. the learning time and the network size while having competitive recognition accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/pasinduwijesena/Zotero/storage/747W939M/Kostadinov et al. - 2019 - Network Parameter Learning Using Nonlinear Transfo.pdf;/Users/pasinduwijesena/Zotero/storage/WJ6NGNRY/1902.html}
}

@misc{MAESTRODataset,
  title = {The {{MAESTRO Dataset}}},
  journal = {Magenta},
  abstract = {MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) is a dataset composed of about 200 hours of virtuosic piano performances captured wit...},
  howpublished = {https://magenta.tensorflow.org/datasets/maestro},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/U2L56WWS/maestro.html}
}

@inproceedings{marsikEvaluationChordChroma2017,
  title = {Evaluation of {{Chord}} and {{Chroma Features}} and {{Dynamic Time Warping Scores}} on {{Cover Song Identification Task}}},
  author = {Mar{\v s}{\'i}k, Ladislav and Rusek, Martin and Slaninov{\'a}, Kate{\v r}ina and Martinovic, Jan and Pokorn{\'y}, Jaroslav},
  year = {2017},
  month = may,
  pages = {205--217},
  doi = {10.1007/978-3-319-59105-6_18},
  abstract = {Cover song identification has been a popular task within music information retrieval in the 20th century. The task is to identify a different version or performance of a previously recorded song. Unlike audio search for an exact matching song, this task has not yet been popularized among users, due to an ambiguous definition of a cover song and the complexity of the problem. With a great variety of methods proposed on the benchmarking challenges, it is increasingly difficult to compare advantages and disadvantages of the features and algorithms. We provide a comparison of three levels of feature extraction (chroma features, chroma vector distances, chord distances) and show how each level affects the results. We further distinguish five scores for dynamic time warping method, to find the best performance in conjunction with the features. Results were evaluated on covers80 and SecondHandSongs datasets and compared to the state-of-the-art.},
  isbn = {978-3-319-59104-9},
  file = {/Users/pasinduwijesena/Zotero/storage/6YV5ZMHF/MarÅ¡Ã­k et al. - 2017 - Evaluation of Chord and Chroma Features and Dynami.pdf}
}

@article{mcfeeSTRUCTUREDTRAININGLARGEVOCABULARY,
  title = {{{STRUCTURED TRAINING FOR LARGE}}-{{VOCABULARY CHORD RECOGNITION}}},
  author = {McFee, Brian and Bello, Juan Pablo},
  pages = {7},
  abstract = {Automatic chord recognition systems operating in the large-vocabulary regime must overcome data scarcity: certain classes occur much less frequently than others, and this presents a significant challenge when estimating model parameters. While most systems model the chord recognition task as a (multi-class) classification problem, few attempts have been made to directly exploit the intrinsic structural similarities between chord classes.},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/93SVXXWW/McFee and Bello - STRUCTURED TRAINING FOR LARGE-VOCABULARY CHORD REC.pdf}
}

@article{miroMASKRobustLocal2012,
  title = {{{MASK}}: {{Robust Local Features}} for {{Audio Fingerprinting}}},
  shorttitle = {{{MASK}}},
  author = {Mir{\'o}, Xavier Anguera and Garzon, Antonio and Adamek, Tomasz},
  year = {2012},
  journal = {2012 IEEE International Conference on Multimedia and Expo},
  doi = {10.1109/ICME.2012.137},
  abstract = {A novel local audio fingerprint called MASK (Masked Audio Spectral Keypoints) that can effectively encode the acoustic information existent in audio documents and discriminate between transformed versions of the same acoustic documents and other unrelated documents is presented. This paper presents a novel local audio fingerprint called MASK (Masked Audio Spectral Keypoints) that can effectively encode the acoustic information existent in audio documents and discriminate between transformed versions of the same acoustic documents and other unrelated documents. The fingerprint has been designed to be resilient to strong transformations of the original signal and to be usable for generic audio, including music and speech. Its main characteristics are its locality, binary encoding, robustness and compactness. The proposed audio fingerprint encodes the local spectral energies around salient points selected among the main spectral peaks in a given signal. Such encoding is done by centering on each point a carefully designed mask defining regions of the spectrogram whose average energies are compared with each other. From each comparison we obtain a single bit depending on which region has more energy, and group all bits into a final binary fingerprint. In addition, the fingerprint also stores the frequency of each peak, quantized using a Mel filterbank. The length of the fingerprint is solely defined by the number of compared regions being used, and can be adapted to the requirements of any particular application. In addition, the number of salient points encoded per second can be also easily modified. In the experimental section we show the suitability of such fingerprint to find matching segments by using the NIST-TRECVID benchmarking evaluation datasets by comparing it with a well known fingerprint, obtaining up to 26\% relative improvement in NDCR score.},
  file = {/Users/pasinduwijesena/Zotero/storage/E8WIKEA2/MirÃ³ et al. - 2012 - MASK Robust Local Features for Audio Fingerprinti.pdf}
}

@misc{MTGDatacos2021,
  title = {{{MTG}}/Da-Tacos},
  year = {2021},
  month = jul,
  abstract = {A Dataset for Cover Song Identification and Understanding},
  copyright = {Apache-2.0},
  howpublished = {Music Technology Group - Universitat Pompeu Fabra},
  keywords = {audio-analysis,cover-song-identification,music-information-retrieval,music-similarity,open-datasets}
}

@article{mullerCrossModalMusicRetrieval2019,
  title = {Cross-{{Modal Music Retrieval}} and {{Applications}}: {{An Overview}} of {{Key Methodologies}}},
  shorttitle = {Cross-{{Modal Music Retrieval}} and {{Applications}}},
  author = {M{\"u}ller, Meinard and Arzt, Andreas and Balke, Stefan and Dorfer, Matthias and Widmer, Gerhard},
  year = {2019},
  month = jan,
  journal = {IEEE Signal Processing Magazine},
  volume = {36},
  number = {1},
  eprint = {1902.04397},
  eprinttype = {arxiv},
  pages = {52--62},
  issn = {1053-5888, 1558-0792},
  doi = {10.1109/MSP.2018.2868887},
  abstract = {There has been a rapid growth of digitally available music data, including audio recordings, digitized images of sheet music, album covers and liner notes, and video clips. This huge amount of data calls for retrieval strategies that allow users to explore large music collections in a convenient way. More precisely, there is a need for cross-modal retrieval algorithms that, given a query in one modality (e.g., a short audio excerpt), find corresponding information and entities in other modalities (e.g., the name of the piece and the sheet music). This goes beyond exact audio identification and subsequent retrieval of metainformation as performed by commercial applications like Shazam [1].},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Multimedia},
  file = {/Users/pasinduwijesena/Zotero/storage/426VU6VK/MÃ¼ller et al. - 2019 - Cross-Modal Music Retrieval and Applications An O.pdf;/Users/pasinduwijesena/Zotero/storage/LRXB8RIP/1902.html}
}

@article{nasrullahMusicArtistClassification2019,
  title = {Music {{Artist Classification}} with {{Convolutional Recurrent Neural Networks}}},
  author = {Nasrullah, Zain and Zhao, Yue},
  year = {2019},
  month = mar,
  journal = {arXiv:1901.04555 [cs, eess, stat]},
  eprint = {1901.04555},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  abstract = {Previous attempts at music artist classification use frame level audio features which summarize frequency content within short intervals of time. Comparatively, more recent music information retrieval tasks take advantage of temporal structure in audio spectrograms using deep convolutional and recurrent models. This paper revisits artist classification with this new framework and empirically explores the impacts of incorporating temporal structure in the feature representation. To this end, an established classification architecture, a Convolutional Recurrent Neural Network (CRNN), is applied to the artist20 music artist identification dataset under a comprehensive set of conditions. These include audio clip length, which is a novel contribution in this work, and previously identified considerations such as dataset split and feature level. Our results improve upon baseline works, verify the influence of the producer effect on classification performance and demonstrate the trade-offs between audio length and training set size. The best performing model achieves an average F1 score of 0.937 across three independent trials which is a substantial improvement over the corresponding baseline under similar conditions. Additionally, to showcase the effectiveness of the CRNN's feature extraction capabilities, we visualize audio samples at the model's bottleneck layer demonstrating that learned representations segment into clusters belonging to their respective artists.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/Users/pasinduwijesena/Zotero/storage/5LRSW4V7/Nasrullah and Zhao - 2019 - Music Artist Classification with Convolutional Rec.pdf;/Users/pasinduwijesena/Zotero/storage/F4E6NFFA/1901.html}
}

@misc{NewStateoftheartVoice,
  title = {A New, State-of-the-Art Voice Separation Model That Distinguishes Multiple Speakers Simultaneously},
  abstract = {We're introducing a new method for separating as many as five voices speaking simultaneously into a single microphone. It pushes state-of-the-art on multiple benchmarks.},
  howpublished = {https://ai.facebook.com/blog/a-new-state-of-the-art-voice-separation-model-that-distinguishes-multiple-speakers-simultaneously/},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/5PB63APX/a-new-state-of-the-art-voice-separation-model-that-distinguishes-multiple-speakers-simultaneous.html}
}

@article{nielsenNeuralNetworksDeep2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  year = {2015},
  publisher = {{Determination Press}},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/V7Q3AY8Z/about.html}
}

@misc{NilandMusicRecommendation,
  title = {Niland - {{Music Recommendation Engine}}},
  howpublished = {http://niland.io/},
  file = {/Users/pasinduwijesena/Zotero/storage/STU6CYX9/niland.io.html}
}

@misc{palmirottaStudyShazamAudio2016,
  title = {A Study of {{Shazam}}'s {{Audio Recognition}}},
  author = {Palmirotta, Guendalina},
  year = {2016},
  month = dec,
  doi = {10.13140/RG.2.2.21768.21766},
  abstract = {Imagine for example you are sitting in a bus, listening to a song and you want to know the artist that is singing this song, respectively the name of this song. Shazam Entertainment Ltd., founded in 2000, is one of the possible applications to recognize the song that are you are listening. You simply start the application on your mobile phone, capture (or tag) the music for a few seconds and after just 10 seconds (after communicating with its remote servers) it will identify the song and display the informations (name, artist and album). This seemed almost magic. The main difficulty is to develop an algorithm that is able to capture by a little cellphone microphone a short audio sample of music often with mixed heavy ambient noise and perform quickly the identification over a large database of music with nearly 2M tracks. Shazam was one of the first to propose such an algorithm that satisfies all these constraints.},
  file = {/Users/pasinduwijesena/Zotero/storage/QV2V6HCK/Palmirotta - 2016 - A study of Shazamâs Audio Recognition.pdf}
}

@misc{PapersCodeMusic,
  title = {Papers with {{Code}} - {{Music Artist Classification}} with {{Convolutional Recurrent Neural Networks}}},
  abstract = {Implemented in 4 code libraries.},
  howpublished = {https://paperswithcode.com/paper/music-artist-classification-with},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/SCCKVXPB/music-artist-classification-with.html}
}

@article{ponsDeepNeuralNetworks,
  title = {Deep Neural Networks for Music and Audio Tagging},
  author = {Pons, Jordi},
  pages = {240},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/KH44WTUX/Pons - Deep neural networks for music and audio tagging.pdf;/Users/pasinduwijesena/Zotero/storage/7D634P9Z/-phd-thesis-deep-neural-networks-for-music-and-audio-tagging.html}
}

@misc{PrincipalComponentAnalysis,
  title = {Principal Component Analysis - {{Abdi}} - 2010 - {{WIREs Computational Statistics}} - {{Wiley Online Library}}},
  howpublished = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.101},
  file = {/Users/pasinduwijesena/Zotero/storage/F5FXAZHN/wics.html}
}

@misc{RecurrentNeuralNetwork,
  title = {Recurrent {{Neural Network}} - an Overview | {{ScienceDirect Topics}}},
  howpublished = {https://www.sciencedirect.com/topics/neuroscience/recurrent-neural-network},
  file = {/Users/pasinduwijesena/Zotero/storage/66VZ3BZ2/recurrent-neural-network.html}
}

@article{reissNonlinearTimeSeries2003,
  title = {Nonlinear {{Time Series Analysis Of Musical Signals}}},
  author = {Reiss, Joshua},
  year = {2003},
  month = aug,
  abstract = {In this work the techniques of chaotic time series analysis are applied to music. The audio stream from musical recordings are treated as representing experimental data from a dynamical system. Several performance of well-known classical pieces are analysed using recurrence analysis, stationarity measures, information metrics, and other time series based approaches. The benefits of such analysis are reported.},
  file = {/Users/pasinduwijesena/Zotero/storage/IWG3N2FC/Reiss - 2003 - Nonlinear Time Series Analysis Of Musical Signals.pdf}
}

@article{robelNeuralNetworkModeling1996,
  title = {Neural {{Network Modeling}} of {{Speech}} and {{Music Signals}}},
  author = {R{\"o}bel, Axel},
  year = {1996},
  pages = {8},
  abstract = {Time series prediction is one of the major applications of neural networks. After a short introduction into the basic theoretical foundations we argue that the iterated prediction of a dynamical system may be interpreted as a model of the system dynamics. By means of RBF neural networks we describe a modeling approach and extend it to be able to model instationary systems. As a practical test for the capabilities of the method we investigate the modeling of musical and speech signals and demonstrate that the model may be used for synthesis of musical and speech signals.},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/F5ZZNPAB/RÃ¶bel - Neural Network Modeling of Speech and Music Signal.pdf}
}

@article{robertsHierarchicalLatentVector2019,
  title = {A {{Hierarchical Latent Vector Model}} for {{Learning Long}}-{{Term Structure}} in {{Music}}},
  author = {Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
  year = {2019},
  month = nov,
  journal = {arXiv:1803.05428 [cs, eess, stat]},
  eprint = {1803.05428},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  abstract = {The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the "posterior collapse" problem, which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a "flat" baseline model. An implementation of our "MusicVAE" is available online at http://g.co/magenta/musicvae-code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/Users/pasinduwijesena/Zotero/storage/HSKRYBRC/Roberts et al. - 2019 - A Hierarchical Latent Vector Model for Learning Lo.pdf;/Users/pasinduwijesena/Zotero/storage/MW8DM7E8/1803.html}
}

@misc{roccaUnderstandingVariationalAutoencoders2021,
  title = {Understanding {{Variational Autoencoders}} ({{VAEs}})},
  author = {Rocca, Joseph},
  year = {2021},
  month = mar,
  journal = {Medium},
  abstract = {Building, step by step, the reasoning that leads to VAEs.},
  howpublished = {https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/779AZHKU/understanding-variational-autoencoders-vaes-f70510919f73.html}
}

@article{saadatpanahAdversarialAttacksCopyright2019,
  title = {Adversarial Attacks on {{Copyright Detection Systems}}},
  author = {Saadatpanah, Parsa and Shafahi, Ali and Goldstein, Tom},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.07153 [cs, stat]},
  eprint = {1906.07153},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {It is well-known that many machine learning models are susceptible to adversarial attacks, in which an attacker evades a classifier by making small perturbations to inputs. This paper discusses how industrial copyright detection tools, which serve a central role on the web, are susceptible to adversarial attacks. We discuss a range of copyright detection systems, and why they are particularly vulnerable to attacks. These vulnerabilities are especially apparent for neural network based systems. As a proof of concept, we describe a well-known music identification method, and implement this system in the form of a neural net. We then attack this system using simple gradient methods. Adversarial music created this way successfully fools industrial systems, including the AudioTag copyright detector and YouTube's Content ID system. Our goal is to raise awareness of the threats posed by adversarial examples in this space, and to highlight the importance of hardening copyright detection systems to attacks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pasinduwijesena/Zotero/storage/CK48F4JF/Saadatpanah et al. - 2019 - Adversarial attacks on Copyright Detection Systems.pdf;/Users/pasinduwijesena/Zotero/storage/BGBGR4EA/1906.html}
}

@article{saadatpanahAdversarialAttacksCopyright2019a,
  title = {Adversarial Attacks on {{Copyright Detection Systems}}},
  author = {Saadatpanah, Parsa and Shafahi, Ali and Goldstein, Tom},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.07153 [cs, stat]},
  eprint = {1906.07153},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {It is well-known that many machine learning models are susceptible to adversarial attacks, in which an attacker evades a classifier by making small perturbations to inputs. This paper discusses how industrial copyright detection tools, which serve a central role on the web, are susceptible to adversarial attacks. We discuss a range of copyright detection systems, and why they are particularly vulnerable to attacks. These vulnerabilities are especially apparent for neural network based systems. As a proof of concept, we describe a well-known music identification method, and implement this system in the form of a neural net. We then attack this system using simple gradient methods. Adversarial music created this way successfully fools industrial systems, including the AudioTag copyright detector and YouTube's Content ID system. Our goal is to raise awareness of the threats posed by adversarial examples in this space, and to highlight the importance of hardening copyright detection systems to attacks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pasinduwijesena/Zotero/storage/2783NIYP/Saadatpanah et al. - 2019 - Adversarial attacks on Copyright Detection Systems.pdf;/Users/pasinduwijesena/Zotero/storage/BIR7QB9C/1906.html}
}

@article{salehinejadRecentAdvancesRecurrent2018,
  title = {Recent {{Advances}} in {{Recurrent Neural Networks}}},
  author = {Salehinejad, Hojjat and Sankar, Sharan and Barfett, Joseph and Colak, Errol and Valaee, Shahrokh},
  year = {2018},
  month = feb,
  journal = {arXiv:1801.01078 [cs]},
  eprint = {1801.01078},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recurrent neural networks (RNNs) are capable of learning features and long term dependencies from sequential and time-series data. The RNNs have a stack of non-linear units where at least one connection between units forms a directed cycle. A well-trained RNN can model any dynamical system; however, training RNNs is mostly plagued by issues in learning long-term dependencies. In this paper, we present a survey on RNNs and several new advances for newcomers and professionals in the field. The fundamentals and recent advances are explained and the research challenges are introduced.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/pasinduwijesena/Zotero/storage/2X9EPEHN/Salehinejad et al. - 2018 - Recent Advances in Recurrent Neural Networks.pdf;/Users/pasinduwijesena/Zotero/storage/8ZIVG9UX/1801.html}
}

@article{schorkhuberCONSTANTQTRANSFORMTOOLBOX,
  title = {{{CONSTANT}}-{{Q TRANSFORM TOOLBOX FOR MUSIC PROCESSING}}},
  author = {Sch{\"o}rkhuber, Christian and Klapuri, Anssi},
  pages = {8},
  abstract = {This paper proposes a computationally efficient method for computing the constant-Q transform (CQT) of a timedomain signal. CQT refers to a time-frequency representation where the frequency bins are geometrically spaced and the Q-factors (ratios of the center frequencies to bandwidths) of all bins are equal. An inverse transform is proposed which enables a reasonable-quality (around 55dB signal-to-noise ratio) reconstruction of the original signal from its CQT coefficients. Here CQTs with high Q-factors, equivalent to 12\textendash 96 bins per octave, are of particular interest. The proposed method is flexible with regard to the number of bins per octave, the applied window function, and the Q-factor, and is particularly suitable for the analysis of music signals. A reference implementation of the proposed methods is published as a Matlab toolbox. The toolbox includes user-interface tools that facilitate spectral data visualization and the indexing and working with the data structure produced by the CQT.},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/7SMS9EJN/SchÃ¶rkhuber and Klapuri - CONSTANT-Q TRANSFORM TOOLBOX FOR MUSIC PROCESSING.pdf}
}

@misc{SecondHandSongsDatasetMillion,
  title = {The {{SecondHandSongs Dataset}} | {{Million Song Dataset}}},
  howpublished = {http://millionsongdataset.com/secondhand/},
  file = {/Users/pasinduwijesena/Zotero/storage/JA5ER3IR/secondhand.html}
}

@incollection{serraAudioCoverSong2010,
  title = {Audio {{Cover Song Identification}} and {{Similarity}}: {{Background}}, {{Approaches}}, {{Evaluation}}, and {{Beyond}}},
  shorttitle = {Audio {{Cover Song Identification}} and {{Similarity}}},
  booktitle = {Studies in {{Computational Intelligence}}},
  author = {Serr{\`a}, Joan and G{\'o}mez, Emilia and Herrera, Perfecto},
  year = {2010},
  month = oct,
  volume = {274},
  pages = {307--332},
  doi = {10.1007/978-3-642-11674-2_14},
  abstract = {A cover version is an alternative rendition of a previously recorded song. Given that a cover may differ from the original song in timbre, tempo, structure, key, arrangement, or language of the vocals, automatically identifying cover songs in a given music collection is a rather difficult task. The music information retrieval (MIR) community has paid much attention to this task in recent years and many approaches have been proposed. This chapter comprehensively summarizes the work done in cover song identification while encompassing the background related to this area of research. The most promising strategies are reviewed and qualitatively compared under a common framework, and their evaluation methodologies are critically assessed. A discussion on the remaining open issues and future lines of research closes the chapter.},
  isbn = {978-3-642-11673-5},
  keywords = {Chord Sequence,Cover Version,Music Collection,Music Information Retrieval,Pitch Class},
  file = {/Users/pasinduwijesena/Zotero/storage/8T9GMC6D/SerrÃ  et al. - 2010 - Audio Cover Song Identification and Similarity Ba.pdf;/Users/pasinduwijesena/Zotero/storage/DE746HQL/SerrÃ  et al. - 2010 - Audio Cover Song Identification and Similarity Ba.pdf}
}

@article{serraChromaBinarySimilarity2008,
  title = {Chroma {{Binary Similarity}} and {{Local Alignment Applied}} to {{Cover Song Identification}}},
  author = {Serra, Joan and G{\'o}mez, Emilia and Herrera, Perfecto and Serra, Xavier},
  year = {2008},
  month = sep,
  journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
  volume = {16},
  pages = {1138--1151},
  doi = {10.1109/TASL.2008.924595},
  abstract = {We present a new technique for audio signal comparison based on tonal subsequence alignment and its application to detect cover versions (i.e., different performances of the same underlying musical piece). Cover song identification is a task whose popularity has increased in the music information retrieval (MIR) community along in the past, as it provides a direct and objective way to evaluate music similarity algorithms. This paper first presents a series of experiments carried out with two state-of-the-art methods for cover song identification. We have studied several components of these (such as chroma resolution and similarity, transposition, beat tracking or dynamic time warping constraints), in order to discover which characteristics would be desirable for a competitive cover song identifier. After analyzing many cross-validated results, the importance of these characteristics is discussed, and the best performing ones are finally applied to the newly proposed method. Multiple evaluations of this one confirm a large increase in identification accuracy when comparing it with alternative state-of-the-art approaches.},
  file = {/Users/pasinduwijesena/Zotero/storage/XVXTAL4K/Serra et al. - 2008 - Chroma Binary Similarity and Local Alignment Appli.pdf}
}

@article{serraCrossRecurrenceQuantification2009,
  title = {Cross Recurrence Quantification for Cover Song Identification},
  author = {Serr{\`a}, Joan and Serra, Xavier and Andrzejak, Ralph G.},
  year = {2009},
  month = sep,
  journal = {New Journal of Physics},
  volume = {11},
  number = {9},
  pages = {093017},
  publisher = {{IOP Publishing}},
  issn = {1367-2630},
  doi = {10.1088/1367-2630/11/9/093017},
  abstract = {There is growing evidence that nonlinear time series analysis techniques can be used to successfully characterize, classify, or process signals derived from real-world dynamics even though these are not necessarily deterministic and stationary. In the present study, we proceed in this direction by addressing an important problem our modern society is facing, the automatic classification of digital information. In particular, we address the automatic identification of cover songs, i.e. alternative renditions of a previously recorded musical piece. For this purpose, we here propose a recurrence quantification analysis measure that allows the tracking of potentially curved and disrupted traces in cross recurrence plots (CRPs). We apply this measure to CRPs constructed from the state space representation of musical descriptor time series extracted from the raw audio signal. We show that our method identifies cover songs with a higher accuracy as compared to previously published techniques. Beyond the particular application proposed here, we discuss how our approach can be useful for the characterization of a variety of signals from different scientific disciplines. We study coupled R\"ossler dynamics with stochastically modulated mean frequencies as one concrete example to illustrate this point.},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/B3MI65EM/SerrÃ  et al. - 2009 - Cross recurrence quantification for cover song ide.pdf}
}

@article{serraUniversalNeuralNetwork2018,
  title = {Towards a Universal Neural Network Encoder for Time Series},
  author = {Serr{\`a}, Joan and Pascual, Santiago and Karatzoglou, Alexandros},
  year = {2018},
  month = may,
  journal = {arXiv:1805.03908 [cs, stat]},
  eprint = {1805.03908},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We study the use of a time series encoder to learn representations that are useful on data set types with which it has not been trained on. The encoder is formed of a convolutional neural network whose temporal output is summarized by a convolutional attention mechanism. This way, we obtain a compact, fixed-length representation from longer, variable-length time series. We evaluate the performance of the proposed approach on a well-known time series classification benchmark, considering full adaptation, partial adaptation, and no adaptation of the encoder to the new data type. Results show that such strategies are competitive with the state-of-the-art, often outperforming conceptually-matching approaches. Besides accuracy scores, the facility of adaptation and the efficiency of pre-trained encoders make them an appealing option for the processing of scarcely- or non-labeled time series.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/pasinduwijesena/Zotero/storage/BP48MBSG/SerrÃ  et al. - 2018 - Towards a universal neural network encoder for tim.pdf;/Users/pasinduwijesena/Zotero/storage/E9ADRAEM/1805.html}
}

@article{serraUniversalNeuralNetwork2018a,
  title = {Towards a Universal Neural Network Encoder for Time Series},
  author = {Serr{\`a}, Joan and Pascual, Santiago and Karatzoglou, Alexandros},
  year = {2018},
  month = may,
  journal = {arXiv:1805.03908 [cs, stat]},
  eprint = {1805.03908},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We study the use of a time series encoder to learn representations that are useful on data set types with which it has not been trained on. The encoder is formed of a convolutional neural network whose temporal output is summarized by a convolutional attention mechanism. This way, we obtain a compact, fixed-length representation from longer, variable-length time series. We evaluate the performance of the proposed approach on a well-known time series classification benchmark, considering full adaptation, partial adaptation, and no adaptation of the encoder to the new data type. Results show that such strategies are competitive with the state-of-the-art, often outperforming conceptually-matching approaches. Besides accuracy scores, the facility of adaptation and the efficiency of pre-trained encoders make them an appealing option for the processing of scarcely- or non-labeled time series.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/pasinduwijesena/Zotero/storage/26PUGUGA/SerrÃ  et al. - 2018 - Towards a universal neural network encoder for tim.pdf;/Users/pasinduwijesena/Zotero/storage/PGIF7CPB/1805.html}
}

@article{serraUnsupervisedDetectionMusic,
  title = {Unsupervised {{Detection}} of {{Music Boundaries}} by {{Time Series Structure Features}}},
  author = {Serra, Joan},
  pages = {7},
  abstract = {Locating boundaries between coherent and/or repetitive segments of a time series is a challenging problem pervading many scientific domains. In this paper we propose an unsupervised method for boundary detection, combining three basic principles: novelty, homogeneity, and repetition. In particular, the method uses what we call structure features, a representation encapsulating both local and global properties of a time series. We demonstrate the usefulness of our approach in detecting music structure boundaries, a task that has received much attention in recent years and for which exist several benchmark datasets and publicly available annotations. We find our method to significantly outperform the best accuracies published so far. Importantly, our boundary approach is generic, thus being applicable to a wide range of time series beyond the music and audio domains.},
  language = {en},
  keywords = {Time Series},
  file = {/Users/pasinduwijesena/Zotero/storage/N4UHERHU/Serra - Unsupervised Detection of Music Boundaries by Time.pdf}
}

@article{serraUnsupervisedMusicStructure2014,
  title = {Unsupervised {{Music Structure Annotation}} by {{Time Series Structure Features}} and {{Segment Similarity}}},
  author = {Serra, Joan and Muller, Meinard and Grosche, Peter and Arcos, Josep Ll.},
  year = {2014},
  month = aug,
  journal = {IEEE Transactions on Multimedia},
  volume = {16},
  number = {5},
  pages = {1229--1240},
  issn = {1520-9210, 1941-0077},
  doi = {10.1109/TMM.2014.2310701},
  abstract = {Automatically inferring the structural properties of raw multimedia documents is essential in today's digitized society. Given its hierarchical and multi-faceted organization, musical pieces represent a challenge for current computational systems. In this article, we present a novel approach to music structure annotation based on the combination of structure features with time series similarity. Structure features encapsulate both local and global properties of a time series, and allow us to detect boundaries between homogeneous, novel, or repeated segments. Time series similarity is used to identify equivalent segments, corresponding to musically meaningful parts. Extensive tests with a total of five benchmark music collections and seven different human annotations show that the proposed approach is robust to different ground truth choices and parameter settings. Moreover, we see that it outperforms previous approaches evaluated under the same framework.},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/LR5JR9I7/Serra et al. - 2014 - Unsupervised Music Structure Annotation by Time Se.pdf}
}

@misc{ShazamMusicDiscovery,
  title = {Shazam - {{Music Discovery}}, {{Charts}} \& {{Song Lyrics}}},
  journal = {Shazam},
  abstract = {Identify the music playing around you. Explore the music you love. Discover songs, lyrics, and artists on Shazam.},
  howpublished = {https://www.shazam.com},
  file = {/Users/pasinduwijesena/Zotero/storage/UFDFZDA6/www.shazam.com.html}
}

@article{sift,
  title = {{{SIFT}}-Based Local Spectrogram Image Descriptor: A Novel Feature for Robust Music Identification},
  shorttitle = {{{SIFT}}-Based Local Spectrogram Image Descriptor},
  author = {Zhang, Xiu and Zhu, Bilei and Li, Linwei and Li, Wei and Li, Xiaoqiang and Wang, Wei and Lu, Peizhong and Zhang, Wenqiang},
  year = {2015},
  month = feb,
  journal = {EURASIP Journal on Audio, Speech, and Music Processing},
  volume = {2015},
  number = {1},
  pages = {6},
  issn = {1687-4722},
  doi = {10.1186/s13636-015-0050-0},
  abstract = {Music identification via audio fingerprinting has been an active research field in recent years. In the real-world environment, music queries are often deformed by various interferences which typically include signal distortions and time-frequency misalignments caused by time stretching, pitch shifting, etc. Therefore, robustness plays a crucial role in music identification technique. In this paper, we propose to use scale invariant feature transform (SIFT) local descriptors computed from a spectrogram image as sub-fingerprints for music identification. Experiments show that these sub-fingerprints exhibit strong robustness against serious time stretching and pitch shifting simultaneously. In addition, a locality sensitive hashing (LSH)-based nearest sub-fingerprint retrieval method and a matching determination mechanism are applied for robust sub-fingerprint matching, which makes the identification efficient and precise. Finally, as an auxiliary function, we demonstrate that by comparing the time-frequency locations of corresponding SIFT keypoints, the factor of time stretching and pitch shifting that music queries might have experienced can be accurately estimated.},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/S9XXALIS/Zhang et al. - 2015 - SIFT-based local spectrogram image descriptor a n.pdf}
}

@misc{SignalProcessingMachine2020,
  title = {Signal {{Processing}} and {{Machine Learning Challenges}} in {{Sound}} and {{Music Computing}}},
  year = {2020},
  month = may,
  abstract = {Presentation by Xavier Serra at ICASSP2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing. https://2020.ieeeicassp.org/program/e...}
}

@article{sixPanakoScalableAcoustic2014,
  title = {Panako - {{A Scalable Acoustic Fingerprinting System Handling Time}}-{{Scale}} and {{Pitch Modification}}},
  author = {Six, Joren and Leman, M.},
  year = {2014},
  journal = {undefined},
  abstract = {A scalable granular acoustic fingerprinting system robust against time and pitch scale modification is presented, designed to be robust against pitch shifting, time stretching and tempo changes, while remaining scalable. In this paper a scalable granular acoustic fingerprinting system robust against time and pitch scale modification is presented. The aim of acoustic fingerprinting is to identify identical, or recognize similar, audio fragments in a large set using condensed representations of audio signals, i.e. fingerprints. A robust fingerprinting system generates similar fingerprints for perceptually similar audio signals. The new system, presented here, handles a variety of distortions well. It is designed to be robust against pitch shifting, time stretching and tempo changes, while remaining scalable. After a query, the system returns the start time in the reference audio, and the amount of pitch shift and tempo change that has been applied. The design of the system that offers this unique combination of features is the main contribution of this research. The fingerprint itself consists of a combination of key points in a Constant-Q spectrogram. The system is evaluated on commodity hardware using a freely available reference database with fingerprints of over 30.000 songs. The results show that the system responds quickly and reliably on queries, while handling time and pitch scale modifications of up to ten percent.},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/QLZXV8UU/Six - PANAKO -A SCALABLE ACOUSTIC FINGERPRINTING SYSTEM .pdf;/Users/pasinduwijesena/Zotero/storage/K3LGMK29/PANAKO_A_SCALABLE_ACOUSTIC_FINGERPRINTING_SYSTEM_HANDLING_TIME_SCALE_AND_PITCH_MODIFICATION.html;/Users/pasinduwijesena/Zotero/storage/S3CCCS9E/b414dcf8ea065825ddf0f4d646e293e5b086f04d.html}
}

@misc{TableComparisonMean,
  title = {Table 3 . {{Comparison}} of Mean Average Rank Score on Covers80 Dataset.},
  journal = {ResearchGate},
  abstract = {Download Table | Comparison of mean average rank score on covers80 dataset. from publication: Evaluation of Chord and Chroma Features and Dynamic Time Warping Scores on Cover Song Identification Task | Cover song identification has been a popular task within music information retrieval in the 20th century. The task is to identify a different version or performance of a previously recorded song. Unlike audio search for an exact matching song, this task has not yet been... | Music, Retrieval and Audio | ResearchGate, the professional network for scientists.},
  howpublished = {https://www.researchgate.net/figure/Comparison-of-mean-average-rank-score-on-covers80-dataset\_tbl1\_317236274},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/8B9PR6VS/Comparison-of-mean-average-rank-score-on-covers80-dataset_tbl1_317236274.html}
}

@article{tian_cheng_comparing_2018,
  title = {Comparing {{RNN Parameters}} for {{Melodic Similarity}}},
  author = {Tian Cheng and Satoru Fukayama and Masataka Goto},
  year = {2018},
  month = sep,
  journal = {Proceedings of the 19th International Society for Music Information Retrieval Conference},
  pages = {763--770},
  publisher = {{ISMIR}},
  doi = {10.5281/zenodo.1492529},
  abstract = {Melodic similarity is an important task in the Music Information Retrieval (MIR) domain, with promising applications including query by example, music recommendation and visualisation. Most current approaches compute the similarity between two melodic sequences by comparing their local features (distance between pitches, intervals, etc.) or by comparing the sequences after aligning them. In order to find a better feature representing global characteristics of a melody, we propose to represent the melodic sequence of each musical piece by the parameters of a generative Recurrent Neural Network (RNN) trained on its sequence. Because the trained RNN can generate the identical melodic sequence of each piece, we can expect that the RNN parameters contain the temporal information within the melody. In our experiment, we first train an RNN on all melodic sequences, and then use it as an initialisation to train an individual RNN on each melodic sequence. The similarity between two melodies is computed by using the distance between their individual RNN parameters. Experimental results showed that the proposed RNN-based similarity outperformed the baseline similarity obtained by directly comparing melodic sequences.},
  keywords = {RNN,Similarity,Time Series},
  file = {/Users/pasinduwijesena/Zotero/storage/2HBNQ4XU/Cheng et al. - 2018 - COMPARING RNN PARAMETERS FOR MELODIC SIMILARITY.pdf;/Users/pasinduwijesena/Zotero/storage/QJNQYZFT/Tian Cheng et al. - 2018 - Comparing RNN Parameters for Melodic Similarity.pdf}
}

@misc{UnderstandingVGG19Architecture2020,
  title = {Understanding the {{VGG19 Architecture}}},
  year = {2020},
  month = feb,
  journal = {OpenGenus IQ: Learn Computer Science},
  abstract = {VGG19 is a variant of VGG model which in short consists of 19 layers (16 convolution layers, 3 Fully connected layer, 5 MaxPool layers and 1 SoftMax layer). There are other variants of VGG like VGG11, VGG16 and others. VGG19 has 19.6 billion FLOPs.},
  howpublished = {https://iq.opengenus.org/vgg19-architecture/},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/BAVGVJ5A/vgg19-architecture.html}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/pasinduwijesena/Zotero/storage/AQSCHEEC/Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@inproceedings{verleysenCurseDimensionalityData2005,
  title = {The {{Curse}} of {{Dimensionality}} in {{Data Mining}} and {{Time Series Prediction}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}}},
  author = {Verleysen, Michel and Fran{\c c}ois, Damien},
  year = {2005},
  month = jun,
  volume = {3512},
  pages = {758--770},
  doi = {10.1007/11494669_93},
  abstract = {Modern data analysis tools have to work on high-dimensional data, whose components are not independently distributed. High-dimensional spaces show surprising, counter-intuitive geometrical properties that have a large influence on the performances of data analysis tools. Among these properties, the concentration of the norm phenomenon results in the fact that Euclidean norms and Gaussian kernels, both commonly used in models, become inappropriate in high-dimensional spaces. This papers presents alternative distance measures and kernels, together with geometrical methods to decrease the dimension of the space. The methodology is applied to a typical time series prediction example.},
  isbn = {978-3-540-26208-4},
  file = {/Users/pasinduwijesena/Zotero/storage/3AU2DE4N/Verleysen and FranÃ§ois - 2005 - The Curse of Dimensionality in Data Mining and Tim.pdf}
}

@misc{VGG11Architecture2020,
  title = {{{VGG}}-11 {{Architecture}}},
  year = {2020},
  month = may,
  journal = {OpenGenus IQ: Learn Computer Science},
  abstract = {In this article, we have explored VGG-11 model architecture which has 11 layers namely 9 Convolution layers (with 5 MaxPool layers), 2 Fully connected layers and an output layer with softmax activation.},
  howpublished = {https://iq.opengenus.org/vgg-11/},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/IIIHJPG2/vgg-11.html}
}

@misc{VGG16Architecture2019,
  title = {{{VGG16}} Architecture},
  year = {2019},
  month = jan,
  journal = {OpenGenus IQ: Learn Computer Science},
  abstract = {VGGNet-16 consists of 16 convolutional layers and is very appealing because of its very uniform Architecture. Similar to AlexNet, it has only 3x3 convolutions, but lots of filters. It can be trained on 4 GPUs for 3 weeks. It is the most preferred choice in the community for extracting image features},
  howpublished = {https://iq.opengenus.org/vgg16/},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/T96DN8C7/vgg16.html}
}

@inproceedings{wangIndustrialStrengthAudio2003,
  title = {An {{Industrial Strength Audio Search Algorithm}}.},
  author = {Wang, Avery},
  year = {2003},
  month = jan,
  file = {/Users/pasinduwijesena/Zotero/storage/YZXB4EF5/Wang - 2003 - An Industrial Strength Audio Search Algorithm..pdf}
}

@article{weerathungaCLASSIFICATIONPUBLICRADIO2019,
  title = {{{CLASSIFICATION OF PUBLIC RADIO BROADCAST CONTEXT FOR ONSET DETECTION}}},
  author = {Weerathunga, C and Jayaratne, Lakshman and Gunawardena, P},
  year = {2019},
  month = dec,
  volume = {7},
  pages = {1--22},
  issn = {2054-0965},
  abstract = {This research focuses on the investigation of a unified methodology for the onset detection in Sri Lankan radio broadcast context with the approach of classification of the broadcast context. Various audio patterns in the broadcast context were observed and a supervised learning approach was employed in the classification process. Different audio features were examined with respect to the broadcast context. Identified audio semantics in the broadcast context were used in refining the output gained in supervised learning models. Onsets were predicted using the classification results. The evaluation method was carried out with ground truth data obtained from a Sri Lankan FM broadcast recording. The proposed approach provided the accuracies of 41\% for news, 76\% for radio commercials, 75\% for songs and 59\% for other voice related segment classification. The onset detection model was successful in predicting the onsets with an error rate of (+/-) 2.5s with approximately 82\% of accuracy level.},
  keywords = {KLJ},
  file = {/Users/pasinduwijesena/Zotero/storage/7WWXIPXF/Weerathunga et al. - 2019 - CLASSIFICATION OF PUBLIC RADIO BROADCAST CONTEXT F.pdf}
}

@article{wold_principal_1987,
  title = {Principal Component Analysis},
  author = {Wold, Svante and Esbensen, Kim and Geladi, Paul},
  year = {1987},
  month = aug,
  journal = {Chemometrics and Intelligent Laboratory Systems},
  series = {Proceedings of the {{Multivariate Statistical Workshop}} for {{Geologists}} and {{Geochemists}}},
  volume = {2},
  number = {1},
  pages = {37--52},
  issn = {0169-7439},
  doi = {10.1016/0169-7439(87)80084-9},
  abstract = {Principal component analysis of a data matrix extracts the dominant patterns in the matrix in terms of a complementary set of score and loading plots. It is the responsibility of the data analyst to formulate the scientific issue at hand in terms of PC projections, PLS regressions, etc. Ask yourself, or the investigator, why the data matrix was collected, and for what purpose the experiments and measurements were made. Specify before the analysis what kinds of patterns you would expect and what you would find exciting. The results of the analysis depend on the scaling of the matrix, which therefore must be specified. Variance scaling, where each variable is scaled to unit variance, can be recommended for general use, provided that almost constant variables are left unscaled. Combining different types of variables warrants blockscaling. In the initial analysis, look for outliers and strong groupings in the plots, indicating that the data matrix perhaps should be ``polished'' or whether disjoint modeling is the proper course. For plotting purposes, two or three principal components are usually sufficient, but for modeling purposes the number of significant components should be properly determined, e.g. by cross-validation. Use the resulting principal components to guide your continued investigation or chemical experimentation, not as an end in itself.},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/89L9BMJJ/Wold et al. - Principal Component Analysis.pdf;/Users/pasinduwijesena/Zotero/storage/CAX5NTAZ/Wold et al. - Principal Component Analysis.pdf;/Users/pasinduwijesena/Zotero/storage/U79YCSAM/Wold et al. - Principal Component Analysis.pdf;/Users/pasinduwijesena/Zotero/storage/QKLNPN5X/0169743987800849.html}
}

@article{xingDistanceMetricLearning,
  title = {Distance Metric Learning, with Application to Clustering with Side-Information},
  author = {Xing, Eric P and Ng, Andrew Y and Jordan, Michael I and Russell, Stuart},
  pages = {8},
  abstract = {Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many ``plausible'' ways, and if a clustering algorithm such as K-means initially fails to find one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufficiently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider ``similar.'' For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in \textcent\textcurrency\textsterling{} , learns a distance metric over \textcent\textyen\textsterling{} that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efficient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to significantly improve clustering performance.},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/Z5G4UB7D/Xing et al. - Distance metric learning, with application to clus.pdf}
}

@article{yangParallelRecurrentConvolutional2020,
  title = {Parallel {{Recurrent Convolutional Neural Networks}}-{{Based Music Genre Classification Method}} for {{Mobile Devices}}},
  author = {Yang, R. and Feng, L. and Wang, H. and Yao, J. and Luo, S.},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {19629--19637},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2968170},
  abstract = {With the rapid development of the mobile internet of things (IoTs) and mobile sensing devices, a large amount of mobile computing-oriented applications have attracted attention both from industry and academia. Deep learning based methods have achieved great success in artificial intelligence (AI) oriented applications. To advance the development of AI-based IoT systems, effective and efficient algorithms are in urgent need for IoT Edge Computing. Time-series data classification is an ongoing problem in applications for mobile devices (e.g. music genre classification on mobile phones). However, the traditional methods require field expertise to extract handcrafted features from the time-series data. Deep learning has been demonstrated to be effective and efficient in this kind of data. Nevertheless, the existing works neglect some of the sequential relationships found in the time-series data, which are significant for time-series data classification. Considering the aforementioned limitations, we propose a hybrid architecture, named the parallel recurrent convolutional neural network (PRCNN). The PRCNN is an end-to-end training network that combines feature extraction and time-series data classification in one stage. The parallel CNN and Bi-RNN blocks focus on extracting the spatial features and temporal frame orders, respectively, and the outputs of two blocks are fused into one powerful representation of the time-series data. Then, the syncretic vector is fed into the softmax function for classification. The parallel network structure guarantees that the extracted features are robust enough to represent the time-series data. Moreover, the experimental results demonstrate that our proposed architecture outperforms the previous approaches applied to the same datasets. We also take the music data as an example to conduct contrastive experiments to verify that our additional parallel Bi-RNN block can improve the performance of time-series classification compared with utilizing CNNs alone.},
  keywords = {AI-based IoT systems,artificial intelligence,Computer architecture,convolutional neural nets,Convolutional neural networks,end-to-end training network,feature extraction,Feature extraction,Internet of Things,IoT edge computing,learning (artificial intelligence),mobile computing,mobile devices,Mobile handsets,mobile Internet of Things,mobile phones,mobile sensing devices,music,Music,music data,music genre classification,neural net architecture,parallel,parallel Bi-RNN,parallel bidirectional recurrent neural network,parallel CNN,parallel processing,parallel recurrent convolutional neural network,pattern classification,PRCNN architecture,recurrent neural nets,Recurrent neural networks,Spectrogram,time series,time-series data classification},
  file = {/Users/pasinduwijesena/Zotero/storage/UKZT9SCP/Yang et al. - 2020 - Parallel Recurrent Convolutional Neural Networks-B.pdf;/Users/pasinduwijesena/Zotero/storage/6PRWLXJS/8964009.html;/Users/pasinduwijesena/Zotero/storage/T3X33ABP/8964009.html}
}

@article{yesilerAccurateScalableVersion2020,
  title = {Accurate and {{Scalable Version Identification Using Musically}}-{{Motivated Embeddings}}},
  author = {Yesiler, Furkan and Serr{\`a}, Joan and G{\'o}mez, Emilia},
  year = {2020},
  month = apr,
  journal = {arXiv:1910.12551 [cs, eess]},
  eprint = {1910.12551},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {The version identification (VI) task deals with the automatic detection of recordings that correspond to the same underlying musical piece. Despite many efforts, VI is still an open problem, with much room for improvement, specially with regard to combining accuracy and scalability. In this paper, we present MOVE, a musically-motivated method for accurate and scalable version identification. MOVE achieves state-of-the-art performance on two publicly-available benchmark sets by learning scalable embeddings in an Euclidean distance space, using a triplet loss and a hard triplet mining strategy. It improves over previous work by employing an alternative input representation, and introducing a novel technique for temporal content summarization, a standardized latent space, and a data augmentation strategy specifically designed for VI. In addition to the main results, we perform an ablation study to highlight the importance of our design choices, and study the relation between embedding dimensionality and model performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/pasinduwijesena/Zotero/storage/FG4CZ8AN/Yesiler et al. - 2020 - Accurate and Scalable Version Identification Using.pdf;/Users/pasinduwijesena/Zotero/storage/HEDXQ8GC/1910.html}
}

@article{yesilerDATACOSDATASETCOVER2019,
  title = {{{DA}}-{{TACOS}}: {{A DATASET FOR COVER SONG IDENTIFICATION AND UNDERSTANDING}}},
  author = {Yesiler, Furkan and Tralie, Chris and Correya, Albin and Silva, Diego F and Tovstogan, Philip and G{\'o}mez, Emilia and Serra, Xavier},
  year = {2019},
  pages = {8},
  abstract = {This paper focuses on Cover Song Identification (CSI), an important research challenge in content-based Music Information Retrieval (MIR). Although the task itself is interesting and challenging for both academia and industry scenarios, there are a number of limitations for the advancement of current approaches. We specifically address two of them in the present study. First, the number of publicly available datasets for this task is limited, and there is no publicly available benchmark set that is widely used among researchers for comparative algorithm evaluation. Second, most of the algorithms are not publicly shared and reproducible, limiting the comparison of approaches. To overcome these limitations we propose Da-TACOS, a DaTAset for COver Song Identification and Understanding, and two frameworks for feature extraction and benchmarking to facilitate reproducibility. Da-TACOS contains 25K songs represented by unique editorial metadata plus 9 low- and mid-level features pre-computed with open source libraries, and is divided into two subsets. The Cover Analysis subset contains audio features (e.g. key, tempo) that can serve to study how musical characteristics vary for cover songs. The Benchmark subset contains the set of features that have been frequently used in CSI research, e.g. chroma, MFCC, beat onsets etc. Moreover, we provide initial benchmarking results of a selected number of state-of-the-art CSI algorithms using our dataset, and for reproducibility, we share a GitHub repository containing the feature extraction and benchmarking frameworks.},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/DS8W6KK3/Yesiler et al. - 2019 - DA-TACOS A DATASET FOR COVER SONG IDENTIFICATION .pdf}
}

@article{yesilerInvestigatingEfficacyMusic2021,
  title = {Investigating the Efficacy of Music Version Retrieval Systems for Setlist Identification},
  author = {Yesiler, Furkan and Molina, Emilio and Serr{\`a}, Joan and G{\'o}mez, Emilia},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.02098 [cs, eess]},
  eprint = {2101.02098},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {The setlist identification (SLI) task addresses a music recognition use case where the goal is to retrieve the metadata and timestamps for all the tracks played in live music events. Due to various musical and non-musical changes in live performances, developing automatic SLI systems is still a challenging task that, despite its industrial relevance, has been under-explored in the academic literature. In this paper, we propose an end-to-end workflow that identifies relevant metadata and timestamps of live music performances using a version identification system. We compare 3 of such systems to investigate their suitability for this particular task. For developing and evaluating SLI systems, we also contribute a new dataset that contains 99.5h of concerts with annotated metadata and timestamps, along with the corresponding reference set. The dataset is categorized by audio qualities and genres to analyze the performance of SLI systems in different use cases. Our approach can identify 68\% of the annotated segments, with values ranging from 35\% to 77\% based on the genre. Finally, we evaluate our approach against a database of 56.8k songs to illustrate the effect of expanding the reference set, where we can still identify 56\% of the annotated segments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/pasinduwijesena/Zotero/storage/I76K6D77/Yesiler et al. - 2021 - Investigating the efficacy of music version retrie.pdf;/Users/pasinduwijesena/Zotero/storage/GVTN482Z/2101.html}
}

@article{yesilerLessMoreFaster2020,
  title = {Less Is More: {{Faster}} and Better Music Version Identification with Embedding Distillation},
  shorttitle = {Less Is More},
  author = {Yesiler, Furkan and Serr{\`a}, Joan and G{\'o}mez, Emilia},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.03284 [cs, eess]},
  eprint = {2010.03284},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Version identification systems aim to detect different renditions of the same underlying musical composition (loosely called cover songs). By learning to encode entire recordings into plain vector embeddings, recent systems have made significant progress in bridging the gap between accuracy and scalability, which has been a key challenge for nearly two decades. In this work, we propose to further narrow this gap by employing a set of data distillation techniques that reduce the embedding dimensionality of a pre-trained state-of-the-art model. We compare a wide range of techniques and propose new ones, from classical dimensionality reduction to more sophisticated distillation schemes. With those, we obtain 99\% smaller embeddings that, moreover, yield up to a 3\% accuracy increase. Such small embeddings can have an important impact in retrieval time, up to the point of making a real-world system practical on a standalone laptop.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/pasinduwijesena/Zotero/storage/WA4VGLWD/WA4VGLWD.pdf;/Users/pasinduwijesena/Zotero/storage/2G7VFF58/2010.html}
}

@techreport{yeSupervisedDeepHashing2019,
  title = {Supervised {{Deep Hashing}} for {{Highly Efficient Cover Song Detection}}},
  author = {Ye, Z. and Choi, J. and Friedland, G.},
  year = {2019},
  month = oct,
  number = {LLNL-CONF-795819},
  institution = {{Lawrence Livermore National Lab. (LLNL), Livermore, CA (United States)}},
  doi = {10.1109/MIPR.2019.00049},
  abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
  language = {English},
  file = {/Users/pasinduwijesena/Zotero/storage/YW58RAWW/Ye et al. - 2019 - Supervised Deep Hashing for Highly Efficient Cover.pdf;/Users/pasinduwijesena/Zotero/storage/LGVBVH3W/1573162.html}
}

@inproceedings{yeSupervisedDeepHashing2019a,
  title = {Supervised {{Deep Hashing}} for {{Highly Efficient Cover Song Detection}}},
  booktitle = {2019 {{IEEE Conference}} on {{Multimedia Information Processing}} and {{Retrieval}} ({{MIPR}})},
  author = {Ye, Zhaoqin and Choi, Jaeyoung and Friedland, Gerald},
  year = {2019},
  month = mar,
  pages = {234--239},
  publisher = {{IEEE}},
  address = {{San Jose, CA, USA}},
  doi = {10.1109/MIPR.2019.00049},
  abstract = {This paper proposes a supervised deep hashing approach for highly efficient and effective cover song detection. Our system consists of two identical sub-neural networks, each one having a hash layer to learn a binary representations of input audio in the form of spectral features. A loss function joins the two outputs of the sub-networks by minimizing the Hamming distance for a pair of audio files covering the same music work. We further enhance system performance by loudness embedding, beat synchronization, and early fusion of input audio features. The output of 128bit hash reaches state-of-the-art performance with mean pairwise accuracy. This system demonstrates the possibility of memory-efficient and real-time efficient cover song detection with satisfiable accuracy in large scale.},
  isbn = {978-1-72811-198-8},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/5A3QMIA8/5A3QMIA8.pdf}
}

@inproceedings{yeSupervisedDeepHashing2019b,
  title = {Supervised {{Deep Hashing}} for {{Highly Efficient Cover Song Detection}}},
  booktitle = {2019 {{IEEE Conference}} on {{Multimedia Information Processing}} and {{Retrieval}} ({{MIPR}})},
  author = {Ye, Zhaoqin and Choi, Jaeyoung and Friedland, Gerald},
  year = {2019},
  month = mar,
  pages = {234--239},
  publisher = {{IEEE}},
  address = {{San Jose, CA, USA}},
  doi = {10.1109/MIPR.2019.00049},
  abstract = {This paper proposes a supervised deep hashing approach for highly efficient and effective cover song detection. Our system consists of two identical sub-neural networks, each one having a hash layer to learn a binary representations of input audio in the form of spectral features. A loss function joins the two outputs of the sub-networks by minimizing the Hamming distance for a pair of audio files covering the same music work. We further enhance system performance by loudness embedding, beat synchronization, and early fusion of input audio features. The output of 128bit hash reaches state-of-the-art performance with mean pairwise accuracy. This system demonstrates the possibility of memory-efficient and real-time efficient cover song detection with satisfiable accuracy in large scale.},
  isbn = {978-1-72811-198-8},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/K9D2W4HQ/Ye et al. - 2019 - Supervised Deep Hashing for Highly Efficient Cover.pdf}
}

@article{yogatamaGenerativeDiscriminativeText2017,
  title = {Generative and {{Discriminative Text Classification}} with {{Recurrent Neural Networks}}},
  author = {Yogatama, Dani and Dyer, Chris and Ling, Wang and Blunsom, Phil},
  year = {2017},
  month = may,
  journal = {arXiv:1703.01898 [cs, stat]},
  eprint = {1703.01898},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We empirically characterize the performance of discriminative and generative LSTM models for text classification. We find that although RNN-based generative models are more powerful than their bag-of-words ancestors (e.g., they account for conditional dependencies across words in a document), they have higher asymptotic error rates than discriminatively trained RNN models. However we also find that generative models approach their asymptotic error rate more rapidly than their discriminative counterparts---the same pattern that Ng \& Jordan (2001) proved holds for linear classification models that make more naive conditional independence assumptions. Building on this finding, we hypothesize that RNN-based generative classification models will be more robust to shifts in the data distribution. This hypothesis is confirmed in a series of experiments in zero-shot and continual learning settings that show that generative models substantially outperform discriminative models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pasinduwijesena/Zotero/storage/WNPJZWEK/Yogatama et al. - 2017 - Generative and Discriminative Text Classification .pdf;/Users/pasinduwijesena/Zotero/storage/SB9VDZY2/1703.html}
}

@article{yu_contrastive_2020,
  title = {Contrastive {{Unsupervised Learning}} for {{Audio Fingerprinting}}},
  author = {Yu, Zhesong and Du, Xingjian and Zhu, Bilei and Ma, Zejun},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.13540 [cs, eess]},
  eprint = {2010.13540},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {The rise of video-sharing platforms has attracted more and more people to shoot videos and upload them to the Internet. These videos mostly contain a carefully-edited background audio track, where serious speech change, pitch shifting and various types of audio effects may involve, and existing audio identification systems may fail to recognize the audio. To solve this problem, in this paper, we introduce the idea of contrastive learning to the task of audio fingerprinting (AFP). Contrastive learning is an unsupervised approach to learn representations that can effectively group similar samples and discriminate dissimilar ones. In our work, we consider an audio track and its differently distorted versions as similar while considering different audio tracks as dissimilar. Based on the momentum contrast (MoCo) framework, we devise a contrastive learning method for AFP, which can generate fingerprints that are both discriminative and robust. A set of experiments showed that our AFP method is effective for audio identification, with robustness to serious audio distortions, including the challenging speed change and pitch shifting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/pasinduwijesena/Zotero/storage/Y9KJDJCN/Yu et al. - 2020 - Contrastive Unsupervised Learning for Audio Finger.pdf;/Users/pasinduwijesena/Zotero/storage/NSD8D7MZ/2010.html}
}

@inproceedings{yuTemporalPyramidPooling2019,
  title = {Temporal {{Pyramid Pooling Convolutional Neural Network}} for {{Cover Song Identification}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Yu, Zhesong and Xu, Xiaoshuo and Chen, Xiaoou and Yang, Deshun},
  year = {2019},
  month = aug,
  pages = {4846--4852},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Macao, China}},
  doi = {10.24963/ijcai.2019/673},
  abstract = {Cover song identification is an important problem in the field of Music Information Retrieval. Most existing methods rely on hand-crafted features and sequence alignment methods, and further breakthrough is hard to achieve. In this paper, Convolutional Neural Networks (CNNs) are used for representation learning toward this task. We show that they could be naturally adapted to deal with key transposition in cover songs. Additionally, Temporal Pyramid Pooling is utilized to extract information on different scales and transform songs with different lengths into fixed-dimensional representations. Furthermore, a training scheme is designed to enhance the robustness of our model. Extensive experiments demonstrate that combined with these techniques, our approach is robust against musical variations existing in cover songs and outperforms state-of-the-art methods on several datasets with low time complexity.},
  isbn = {978-0-9992411-4-1},
  language = {en},
  file = {/Users/pasinduwijesena/Zotero/storage/DCVQAHSA/Yu et al. - 2019 - Temporal Pyramid Pooling Convolutional Neural Netw.pdf}
}


