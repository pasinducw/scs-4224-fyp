{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from SAMAF import SAMAF\n",
    "# from SinhalaSongsDataset import SinhalaSongsDataset\n",
    "from EvaluationSinhalaSongsDataset import EvaluationSinhalaSongsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_mfccs(mfccs):\n",
    "    plt.figure()\n",
    "    librosa.display.specshow(mfccs.transpose(0,1).numpy(), x_axis=\"time\")\n",
    "    plt.colorbar()\n",
    "    plt.title(\"MFCC\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index(dataloader, model_params, device):\n",
    "    model = SAMAF(embedding_dim=196).to(device)\n",
    "    model.load_state_dict(model_params)\n",
    "\n",
    "    index = []\n",
    "\n",
    "    def threshold(value):\n",
    "        if value > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (song_ids, mfccs) in enumerate(dataloader):\n",
    "            embeddings, _ = model(mfccs)\n",
    "            embeddings = embeddings.detach().cpu().numpy()\n",
    "            embeddings = np.vectorize(threshold)(embeddings).astype(bool)\n",
    "            for j, song_id in enumerate(song_ids):\n",
    "                for offset, embedding in enumerate(embeddings[j]):\n",
    "                    index.append((embedding, song_id, offset))\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model_parms, device, index):\n",
    "    model = SAMAF(embedding_dim=196).to(device)\n",
    "    model.load_state_dict(model_parms)\n",
    "\n",
    "    def threshold(value):\n",
    "        if value > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def compute_and_get_best_matchings(index, hash):\n",
    "        matchings = []\n",
    "        best = 0\n",
    "        for (key, music_id, _) in index:\n",
    "            score = len(hash) - np.sum(np.logical_xor(hash, key))\n",
    "            best = max(best, score)\n",
    "            matchings.append((score, music_id))\n",
    "        \n",
    "        best_matchings = []\n",
    "        for (score, music_id) in matchings:\n",
    "            if score == best:\n",
    "                best_matchings.append((score,music_id))\n",
    "        return np.array(best_matchings)\n",
    "\n",
    "    vectorized_threshold = np.vectorize(threshold)\n",
    "    correct_matches = 0\n",
    "    incorrect_matches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (song_ids, mfccs) in enumerate(dataloader):\n",
    "            embeddings, _ = model(mfccs)\n",
    "            embeddings = embeddings.detach().cpu().numpy()\n",
    "            embeddings = vectorized_threshold(embeddings).astype(bool)\n",
    "            for j, song_id in enumerate(song_ids):\n",
    "                candidate_matchings = []\n",
    "                for _, embedding in enumerate(embeddings[j]):\n",
    "                    matchings = compute_and_get_best_matchings(index, embedding)\n",
    "                    matchings = np.unique(np.array(matchings)[:,1])\n",
    "                    candidate_matchings.extend(matchings)\n",
    "                candidate_matchings = np.array(candidate_matchings)\n",
    "                (matching_song_ids, counts) = np.unique(candidate_matchings, return_counts=True)\n",
    "                matched_song_id = matching_song_ids[np.argmax(counts)]\n",
    "                if song_id == matched_song_id:\n",
    "                    correct_matches += 1\n",
    "                else:\n",
    "                    incorrect_matches += 1\n",
    "            print(\"Current Accuracy\", correct_matches/(correct_matches+incorrect_matches))\n",
    "    return correct_matches, incorrect_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "checkpoint = torch.load(\"../data/L1-D196-B20-E100-EXP2/snapshot-10.pytorch\", map_location=device)\n",
    "features_directory = \"/home/pasinducw/Downloads/Research-Datasets/Sinhala-Songs/features\"\n",
    "\n",
    "index_dataset = EvaluationSinhalaSongsDataset(root_dir=features_directory, trim_seconds=10, indexing=True)\n",
    "index_dataloader = torch.utils.data.DataLoader(index_dataset, shuffle=False)\n",
    "\n",
    "query_dataset = EvaluationSinhalaSongsDataset(root_dir=features_directory, trim_seconds=10)\n",
    "query_dataloader = torch.utils.data.DataLoader(query_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the index\n",
    "index = make_index(index_dataloader, checkpoint['best_model_weights'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7900, 196)\n",
      "(2908, 196)\n"
     ]
    }
   ],
   "source": [
    "hashes = map(lambda x: x[0], index)\n",
    "hashes = np.array(list(hashes))\n",
    "unique = np.unique(hashes, axis=0)\n",
    "\n",
    "print(hashes.shape)\n",
    "print(unique.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(query_dataloader, checkpoint['best_model_weights'], device, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
